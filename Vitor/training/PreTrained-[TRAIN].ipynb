{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0\n",
    "!pip install pytorch_lightning==2.1\n",
    "!pip install pandas librosa opencv-python matplotlib  #cupy-cuda110 \n",
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal as sci_signal\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import albumentations as albu\n",
    "from torchvision.models import efficientnet\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    \n",
    "    # == GENERAL ==\n",
    "    seed = 1917                             # random seed\n",
    "    device = 'cuda'                         # device to be used\n",
    "    mix_precision = False                   # whether to use mixed-16 precision\n",
    "    \n",
    "    # == DATA ==\n",
    "    use_holdout = False                                                  # Creates a holdout set\n",
    "    holdout_path = '../../input/birdclef-2024/holdout_55bottom_20.0%_ratleq3_.csv'  # Path of files from holdout (generated with CreateHoldout.ipynb)\n",
    "    holdout_noise_val = [0.01, 0.02]                                    # Adds noise to the holdout specs if using holdout ḿin_var, max_var]\n",
    "\n",
    "    preprocess = True                                                  # If true, will recalculated all center specs for the training set\n",
    "    preprocessed_data = 'VictorFeatures/'                                  # Path for processed data to be stores (Must put on .gitignore to not send to repo)\n",
    "    checkpoint_dir = '../../input/efficient-net-1foldstrat-CEBAL'          # Checkpoints path (Must put on .gitignore to not send to repo)\n",
    "    data_dir_2024 = '../../input/birdclef-2024'# root folder\n",
    "    sr = 32000                              # sampling rate\n",
    "    n_fft = 1095                            # NFFT of Spec.\n",
    "    win_len = 412                           # WIN_SIZE of Spec.\n",
    "    hop_len = 100                           # overlap of Spec.\n",
    "    min_freq = 40                           # min frequency\n",
    "    max_freq = 15000                        # max frequency\n",
    "    \n",
    "    # == MODEL ==\n",
    "    model = 'efficientnet_b0'               # model architecture\n",
    "    \n",
    "    # == DATASET ==\n",
    "    batch_size = 32                         # batch size of each step\n",
    "    n_workers = 4                           # number of workers\n",
    "    \n",
    "    # == AUG ==\n",
    "    USE_HORIZFLIP = True\n",
    "    USE_XYMASKING = True                    # whether use XYMasking\n",
    "\n",
    "    # == TRAINING ==\n",
    "    folds = 3                               # n fold\n",
    "    epochs = 1                              # max epochs\n",
    "    lr = 1e-3                               # learning rate\n",
    "    weight_decay = 1e-5                     # weight decay of optimizer\n",
    "    visualize = True                        # whether to visualize data and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device, ', Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "pl.seed_everything(CONFIG.seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kagle Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    '''\n",
    "    Version of macro-averaged ROC-AUC score that ignores all classes that have no true positive labels.\n",
    "    '''\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    if not pandas.api.types.is_numeric_dtype(submission.values):\n",
    "        bad_dtypes = {x: submission[x].dtype  for x in submission.columns if not pandas.api.types.is_numeric_dtype(submission[x])}\n",
    "        raise ParticipantVisibleError(f'Invalid submission data types found: {bad_dtypes}')\n",
    "\n",
    "    solution_sums = solution.sum(axis=0)\n",
    "    scored_columns = list(solution_sums[solution_sums > 0].index.values)\n",
    "    assert len(scored_columns) > 0\n",
    "\n",
    "    return sklearn.metrics.roc_auc_score(solution[scored_columns].values, submission[scored_columns].values, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which data is gonna be used for both Train-Val and Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ 2024 TRAIN DATAFRAME AND ADD LABELS\n",
    "df = pd.read_csv(f'{CONFIG.data_dir_2024}/train_metadata.csv')\n",
    "df.head()\n",
    "label_list = sorted(df['primary_label'].unique())\n",
    "label_id_list = list(range(len(label_list)))\n",
    "label2id = dict(zip(label_list, label_id_list))\n",
    "id2label = dict(zip(label_id_list, label_list))\n",
    "df = df[['primary_label', 'rating', 'filename']].copy() \n",
    "df['target'] = df.primary_label.map(label2id)\n",
    "df['filepath'] = CONFIG.data_dir_2024 + '/train_audio/' + df.filename\n",
    "df['name'] = df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "\n",
    "# CREATE HOLDOUT\n",
    "if CONFIG.use_holdout:\n",
    "    holdout_df = pd.read_csv(CONFIG.holdout_path)\n",
    "    holdout_df = df[df['name'].isin(holdout_df['name'])] \n",
    "    train_df = df[~df['name'].isin(holdout_df['name'])] \n",
    "    print('> #Holdout Samples: ', len(holdout_df))\n",
    "    print('> #Training Samples:', len(train_df))\n",
    "    print(holdout_df)\n",
    "    print(train_df)\n",
    "else:\n",
    "    train_df = df\n",
    "    print('> #Training Samples:', len(train_df))\n",
    "    print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates Image Spectrograms, for both train-val (audio center), and holdout (every 5 seconds of audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(filepath):\n",
    "\n",
    "    # LOAD .OGG\n",
    "    audio_data, _ = librosa.load(filepath, sr=CONFIG.sr)\n",
    "\n",
    "    # CROP CENTER 5 SECONDS,  OR COPY N_COPY TIMES IF <5s (UNTIL 5s is achieved)\n",
    "    n_copy = math.ceil(5 * CONFIG.sr / len(audio_data))\n",
    "    if n_copy > 1: audio_data = np.concatenate([audio_data]*n_copy)\n",
    "    start_idx = int(len(audio_data) / 2 - 2.5 * CONFIG.sr)\n",
    "    end_idx = int(start_idx + 5.0 * CONFIG.sr)\n",
    "    input_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "    # HANDLE NaNs\n",
    "    mean_signal = np.nanmean(input_audio)\n",
    "    input_audio = np.nan_to_num(input_audio, nan=mean_signal) if np.isnan(input_audio).mean() < 1 else np.zeros_like(input_audio)\n",
    "    \n",
    "    # SPECTROGRAM\n",
    "    frequencies, times, spec_data = sci_signal.spectrogram(\n",
    "        input_audio, \n",
    "        fs=CONFIG.sr, \n",
    "        nfft=CONFIG.n_fft, \n",
    "        nperseg=CONFIG.win_len, \n",
    "        noverlap=CONFIG.hop_len, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    # FILTER LOWER AND HIGHER FREQUENCIES\n",
    "    valid_freq = (frequencies >= CONFIG.min_freq) & (frequencies <= CONFIG.max_freq)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # COMPUTE LOG SPEC\n",
    "    spec_data = np.log10(spec_data + 1e-20)\n",
    "    \n",
    "    # MIN/MAX NORMALIZATION\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "\n",
    "    # SPEC TO IMAGE\n",
    "    spec_data = cv2.resize(spec_data, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return spec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS TRAIN\n",
    "if not CONFIG.preprocess:\n",
    "    all_bird_data = np.load(f'{CONFIG.preprocessed_data}/spec_center_5sec_256_256.npy', allow_pickle=True).item()\n",
    "else:\n",
    "    all_bird_data = dict()\n",
    "    for i, row_metadata in tqdm(train_df.iterrows()):\n",
    "        \n",
    "        # filepath to processed spec img\n",
    "        processed_spec = preprocess_fn(row_metadata.filepath)\n",
    "\n",
    "        all_bird_data[row_metadata.name] = processed_spec.astype(np.float32)\n",
    "\n",
    "    # save to file\n",
    "    np.save(os.path.join(CONFIG.preprocessed_data, f'spec_center_5sec_256_256.npy'), all_bird_data)\n",
    "\n",
    "samples_per_class = train_df['target'].value_counts().sort_index().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oog2spec_via_scipy(audio_data):\n",
    "    # handles NaNs\n",
    "    mean_signal = np.nanmean(audio_data)\n",
    "    audio_data = np.nan_to_num(audio_data, nan=mean_signal) if np.isnan(audio_data).mean() < 1 else np.zeros_like(audio_data)\n",
    "    \n",
    "    # to spec.\n",
    "    frequencies, times, spec_data = sci_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=CONFIG.sr, \n",
    "        nfft=CONFIG.n_fft, \n",
    "        nperseg=CONFIG.win_len, \n",
    "        noverlap=CONFIG.hop_len, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    # Filter frequency range\n",
    "    valid_freq = (frequencies >= CONFIG.min_freq) & (frequencies <= CONFIG.max_freq)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # Log\n",
    "    spec_data = np.log10(spec_data + 1e-20)\n",
    "    \n",
    "    # min/max normalize\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS HOLDOUT\n",
    "if not CONFIG.use_holdout:\n",
    "    print('entrou')\n",
    "    all_bird_data_holdout = dict()\n",
    "    temp_df = pd.DataFrame()\n",
    "    for i, row in tqdm(holdout_df.iterrows()):\n",
    "        row_id = row['name']\n",
    "        file_path = row['filepath']\n",
    "        audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "        spec = oog2spec_via_scipy(audio_data)\n",
    "        pad = 512 - (spec.shape[1] % 512)\n",
    "        if pad > 0:\n",
    "            spec = np.pad(spec, ((0,0), (0,pad)))\n",
    "        spec = spec.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "        spec = cv2.resize(spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "        if len(spec.shape) == 3:\n",
    "            for j in range(spec.shape[-1]):\n",
    "                all_bird_data_holdout[f'{row_id}_{(j+1)*5}'] = spec[:, :, j]\n",
    "                temp_row = row\n",
    "                temp_row['name'] = f'{row_id}_{(j+1)*5}'\n",
    "                temp_df = pd.concat([temp_df, temp_row.to_frame().transpose()])\n",
    "        else:\n",
    "            j=0\n",
    "            all_bird_data_holdout[f'{row_id}_{(j+1)*5}'] = spec[:, :]\n",
    "            temp_row = row\n",
    "            temp_row['name'] = f'{row_id}_{(j+1)*5}'\n",
    "            temp_df = pd.concat([temp_df, temp_row.to_frame().transpose()])\n",
    "    holdout_df = temp_df\n",
    "    print('> #Holdout Samples: ', len(all_bird_data_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata,\n",
    "        data,\n",
    "        augmentation=None,\n",
    "        holdout_flag=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.augmentation = augmentation\n",
    "        self.data = data\n",
    "        self.holdout_flag = holdout_flag\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row_metadata = self.metadata.iloc[index]\n",
    "        \n",
    "        # load spec. data\n",
    "        if self.holdout_flag:\n",
    "            input_spec = self.data[row_metadata['name']]\n",
    "        else:\n",
    "            input_spec = self.data[row_metadata.name]\n",
    "        # aug\n",
    "        if self.augmentation is not None:\n",
    "            input_spec = self.augmentation(image=input_spec)['image']\n",
    "        \n",
    "        # target\n",
    "        target = row_metadata.target\n",
    "        \n",
    "        return torch.tensor(input_spec, dtype=torch.float32), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(_type):\n",
    "    \n",
    "    if _type == 'train':\n",
    "        return albu.Compose([\n",
    "            albu.HorizontalFlip(0.5) if CONFIG.USE_XYMASKING else albu.NoOp(),\n",
    "            albu.XYMasking(\n",
    "                p=0.3,\n",
    "                num_masks_x=(1, 3),\n",
    "                num_masks_y=(1, 3),\n",
    "                mask_x_length=(1, 10),\n",
    "                mask_y_length=(1, 20),\n",
    "            ) if CONFIG.USE_XYMASKING else albu.NoOp()\n",
    "        ])\n",
    "    elif _type == 'valid':\n",
    "        return albu.Compose([])\n",
    "    elif _type == 'test':\n",
    "        return albu.Compose([\n",
    "            albu.GaussNoise(var_limit=(CONFIG.holdout_noise_val[0], CONFIG.holdout_noise_val[1]), mean=0)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(ds, row=3, col=3):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    img_index = np.random.randint(0, len(ds)-1, row*col)\n",
    "    \n",
    "    for i in range(len(img_index)):\n",
    "        img, label = dummy_dataset[img_index[i]]\n",
    "        \n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        \n",
    "        ax = fig.add_subplot(row, col, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(img, cmap='jet')\n",
    "        ax.set_title(f'ID: {img_index[i]}; Target: {label}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset = BirdDataset(train_df, all_bird_data, get_transforms('train'))\n",
    "\n",
    "test_input, test_target = dummy_dataset[0]\n",
    "print(test_input.detach().numpy().shape)\n",
    "\n",
    "show_batch(dummy_dataset)\n",
    "\n",
    "del dummy_dataset\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.use_holdout:\n",
    "    dummy_dataset = BirdDataset(holdout_df, all_bird_data_holdout, get_transforms('test'), True)\n",
    "\n",
    "    test_input, test_target = dummy_dataset[0]\n",
    "    print(test_input.detach().numpy().shape)\n",
    "\n",
    "    show_batch(dummy_dataset)\n",
    "\n",
    "    del dummy_dataset\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>filepath</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>5.0</td>\n",
       "      <td>asbfly/XC134896.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/asbfly/X...</td>\n",
       "      <td>asbfly-XC134896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>2.5</td>\n",
       "      <td>asbfly/XC164848.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/asbfly/X...</td>\n",
       "      <td>asbfly-XC164848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>2.5</td>\n",
       "      <td>asbfly/XC175797.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/asbfly/X...</td>\n",
       "      <td>asbfly-XC175797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>4.0</td>\n",
       "      <td>asbfly/XC207738.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/asbfly/X...</td>\n",
       "      <td>asbfly-XC207738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>4.0</td>\n",
       "      <td>asbfly/XC209218.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/asbfly/X...</td>\n",
       "      <td>asbfly-XC209218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24454</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>zitcis1/XC845747.ogg</td>\n",
       "      <td>181</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/zitcis1/...</td>\n",
       "      <td>zitcis1-XC845747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24455</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>zitcis1/XC845817.ogg</td>\n",
       "      <td>181</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/zitcis1/...</td>\n",
       "      <td>zitcis1-XC845817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24456</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>zitcis1/XC856176.ogg</td>\n",
       "      <td>181</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/zitcis1/...</td>\n",
       "      <td>zitcis1-XC856176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24457</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>zitcis1/XC856723.ogg</td>\n",
       "      <td>181</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/zitcis1/...</td>\n",
       "      <td>zitcis1-XC856723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24458</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>zitcis1/XC858550.ogg</td>\n",
       "      <td>181</td>\n",
       "      <td>../../input/birdclef-2024/train_audio/zitcis1/...</td>\n",
       "      <td>zitcis1-XC858550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24197 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary_label  rating              filename  target  \\\n",
       "0            asbfly     5.0   asbfly/XC134896.ogg       0   \n",
       "1            asbfly     2.5   asbfly/XC164848.ogg       0   \n",
       "2            asbfly     2.5   asbfly/XC175797.ogg       0   \n",
       "3            asbfly     4.0   asbfly/XC207738.ogg       0   \n",
       "4            asbfly     4.0   asbfly/XC209218.ogg       0   \n",
       "...             ...     ...                   ...     ...   \n",
       "24454       zitcis1     5.0  zitcis1/XC845747.ogg     181   \n",
       "24455       zitcis1     4.0  zitcis1/XC845817.ogg     181   \n",
       "24456       zitcis1     4.0  zitcis1/XC856176.ogg     181   \n",
       "24457       zitcis1     4.5  zitcis1/XC856723.ogg     181   \n",
       "24458       zitcis1     5.0  zitcis1/XC858550.ogg     181   \n",
       "\n",
       "                                                filepath              name  \n",
       "0      ../../input/birdclef-2024/train_audio/asbfly/X...   asbfly-XC134896  \n",
       "1      ../../input/birdclef-2024/train_audio/asbfly/X...   asbfly-XC164848  \n",
       "2      ../../input/birdclef-2024/train_audio/asbfly/X...   asbfly-XC175797  \n",
       "3      ../../input/birdclef-2024/train_audio/asbfly/X...   asbfly-XC207738  \n",
       "4      ../../input/birdclef-2024/train_audio/asbfly/X...   asbfly-XC209218  \n",
       "...                                                  ...               ...  \n",
       "24454  ../../input/birdclef-2024/train_audio/zitcis1/...  zitcis1-XC845747  \n",
       "24455  ../../input/birdclef-2024/train_audio/zitcis1/...  zitcis1-XC845817  \n",
       "24456  ../../input/birdclef-2024/train_audio/zitcis1/...  zitcis1-XC856176  \n",
       "24457  ../../input/birdclef-2024/train_audio/zitcis1/...  zitcis1-XC856723  \n",
       "24458  ../../input/birdclef-2024/train_audio/zitcis1/...  zitcis1-XC858550  \n",
       "\n",
       "[24197 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105378/1560425894.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['fold'] = 0\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=CONFIG.folds, shuffle=True, random_state=CONFIG.seed)\n",
    "train_df['fold'] = 0\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, y=train_df['primary_label'])):\n",
    "    for idx in val_idx:\n",
    "        train_df.iloc[idx, train_df.columns.get_loc('fold') ] = fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type, n_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if model_type == 'efficientnet_b0':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B0_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b0(weights=weights)\n",
    "        elif model_type == 'efficientnet_b1':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B1_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b1(weights=weights)\n",
    "        elif model_type == 'efficientnet_b2':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B2_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b2(weights=weights)\n",
    "        elif model_type == 'efficientnet_b3':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B3_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b3(weights=weights)\n",
    "        else:\n",
    "            raise ValueError('model type not supported')\n",
    "        \n",
    "        self.base_model.classifier[1] = nn.Linear(self.base_model.classifier[1].in_features, n_classes, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.cat([x, x, x], dim=3).permute(0, 3, 1, 2)\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 86.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 182])\n"
     ]
    }
   ],
   "source": [
    "dummy_model = EffNet(CONFIG.model, n_classes=len(label_list))\n",
    "dummy_input = torch.randn(2, 256, 256)\n",
    "print(dummy_model(dummy_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, labels, alpha=None, gamma=2):\n",
    "    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n",
    "    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n",
    "    where pt is the probability of being classified to the true class.\n",
    "    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n",
    "    Args:\n",
    "      logits: A float tensor of size [batch, num_classes].\n",
    "      labels: A float tensor of size [batch, num_classes].\n",
    "      alpha: A float tensor of size [batch_size]\n",
    "        specifying per-example weight for balanced cross entropy.\n",
    "      gamma: A float scalar modulating loss from hard and easy examples.\n",
    "    Returns:\n",
    "      focal_loss: A float32 scalar representing normalized total loss.\n",
    "    \"\"\"\n",
    "    bc_loss = torch.nn.functional.binary_cross_entropy_with_logits(input=logits, target=labels, reduction=\"none\")\n",
    "\n",
    "    if gamma == 0.0:\n",
    "        modulator = 1.0\n",
    "    else:\n",
    "        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + torch.exp(-1.0 * logits)))\n",
    "\n",
    "    loss = modulator * bc_loss\n",
    "\n",
    "    if alpha is not None:\n",
    "        weighted_loss = alpha * loss\n",
    "        focal_loss = torch.sum(weighted_loss)\n",
    "    else:\n",
    "        focal_loss = torch.sum(loss)\n",
    "\n",
    "    focal_loss /= torch.sum(labels)\n",
    "    return focal_loss\n",
    "\n",
    "class BalancedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples_per_class=None,\n",
    "        loss_type: str = \"cross_entropy\",\n",
    "        beta: float = 0.999,\n",
    "        fl_gamma=2,\n",
    "        class_balanced=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n",
    "        Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
    "        where Loss is one of the standard losses used for Neural Networks.\n",
    "        reference: https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf\n",
    "        \n",
    "        Example usage:\n",
    "        \n",
    "            # outputs and labels\n",
    "            logits = torch.tensor([[0.78, 0.1, 0.05]]) # 1 batch, 3 class\n",
    "            labels = torch.tensor([0]) # 1 batch\n",
    "\n",
    "            # number of samples per class in the training dataset\n",
    "            samples_per_class = [30, 100, 25] # 30, 100, 25 samples for labels 0, 1 and 2, respectively\n",
    "\n",
    "            # class-balanced focal loss\n",
    "            focal_loss = Loss(\n",
    "                loss_type=\"focal_loss\",\n",
    "                samples_per_class=samples_per_class,\n",
    "                class_balanced=True\n",
    "            )\n",
    "            \n",
    "            loss = focal_loss(logits, labels)\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "            loss_type: string. One of \"focal_loss\", \"cross_entropy\",\n",
    "                \"binary_cross_entropy\", \"softmax_binary_cross_entropy\".\n",
    "            beta: float. Hyperparameter for Class balanced loss.\n",
    "            fl_gamma: float. Hyperparameter for Focal loss.\n",
    "            samples_per_class: A python list of size [num_classes].\n",
    "                Required if class_balance is True.\n",
    "            class_balanced: bool. Whether to use class balanced loss.\n",
    "        Returns:\n",
    "            BalancedCrossEntropyLoss instance\n",
    "        \"\"\"\n",
    "        super(BalancedCrossEntropyLoss, self).__init__()\n",
    "\n",
    "        if class_balanced is True and samples_per_class is None:\n",
    "            raise ValueError(\"samples_per_class cannot be None when class_balanced is True\")\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "        self.beta = beta\n",
    "        self.fl_gamma = fl_gamma\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.class_balanced = class_balanced\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.tensor,\n",
    "        labels: torch.tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n",
    "        Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
    "        where Loss is one of the standard losses used for Neural Networks.\n",
    "        Args:\n",
    "            logits: A float tensor of size [batch, num_classes].\n",
    "            labels: An int tensor of size [batch].\n",
    "        Returns:\n",
    "            cb_loss: A float tensor representing class balanced loss\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = logits.size(0)\n",
    "        num_classes = logits.size(1)\n",
    "        labels_one_hot = torch.nn.functional.one_hot(labels, num_classes).float()\n",
    "\n",
    "        if self.class_balanced:\n",
    "            effective_num = 1.0 - np.power(self.beta, self.samples_per_class)\n",
    "            weights = (1.0 - self.beta) / np.array(effective_num)\n",
    "            weights = weights / np.sum(weights) * num_classes\n",
    "            weights = torch.tensor(weights, device=logits.device).float()\n",
    "\n",
    "            if self.loss_type != \"cross_entropy\":\n",
    "                weights = weights.unsqueeze(0)\n",
    "                weights = weights.repeat(batch_size, 1) * labels_one_hot\n",
    "                weights = weights.sum(1)\n",
    "                weights = weights.unsqueeze(1)\n",
    "                weights = weights.repeat(1, num_classes)\n",
    "        else:\n",
    "            weights = None\n",
    "\n",
    "        if self.loss_type == \"focal_loss\":\n",
    "            cb_loss = focal_loss(logits, labels_one_hot, alpha=weights, gamma=self.fl_gamma)\n",
    "        elif self.loss_type == \"cross_entropy\":\n",
    "            cb_loss = torch.nn.functional.cross_entropy(input=logits, target=labels_one_hot, weight=weights)\n",
    "        elif self.loss_type == \"binary_cross_entropy\":\n",
    "            cb_loss = torch.nn.functional.binary_cross_entropy_with_logits(input=logits, target=labels_one_hot, weight=weights)\n",
    "        elif self.loss_type == \"softmax_binary_cross_entropy\":\n",
    "            pred = logits.softmax(dim=1)\n",
    "            cb_loss = torch.nn.functional.binary_cross_entropy(input=pred, target=labels_one_hot, weight=weights)\n",
    "        return cb_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        self.backbone = EffNet(CONFIG.model, n_classes=len(label_list))\n",
    "        \n",
    "        # == loss function ==\n",
    "        self.loss_fn = BalancedCrossEntropyLoss(samples_per_class=samples_per_class)\n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=CONFIG.lr,\n",
    "            weight_decay=CONFIG.weight_decay\n",
    "        )\n",
    "        \n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=CONFIG.epochs,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        \n",
    "        # target to one-hot\n",
    "        target_val = torch.nn.functional.one_hot(target_val, len(label_list))\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "        \n",
    "        self.log(\"val_score\", val_score, True)\n",
    "        \n",
    "        # clear validation outputs\n",
    "        self.validation_step_outputs = list()\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_score': val_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    model.to(CONFIG.device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch\n",
    "            x = x.cuda()\n",
    "            outputs = model(x)\n",
    "            outputs = nn.Softmax(dim=1)(outputs)\n",
    "        predictions.append(outputs.detach().cpu())\n",
    "        gts.append(y.detach().cpu())\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0).cpu().detach()\n",
    "    gts = torch.cat(gts, dim=0).cpu().detach()\n",
    "    gts = torch.nn.functional.one_hot(gts, len(label_list))\n",
    "    return predictions.numpy().astype(np.float32), gts.numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, total_df):\n",
    "    print('================================================================')\n",
    "    print(f\"==== Running training for fold {fold_id} ====\")\n",
    "    \n",
    "    # == create dataset and dataloader ==\n",
    "    train_df = total_df[total_df['fold'] != fold_id].copy()\n",
    "    valid_df = total_df[total_df['fold'] == fold_id].copy()\n",
    "    \n",
    "    print(f'Train Samples: {len(train_df)}')\n",
    "    print(f'Valid Samples: {len(valid_df)}')\n",
    "    \n",
    "    train_ds = BirdDataset(train_df, all_bird_data, get_transforms('train'), False)\n",
    "    val_ds = BirdDataset(valid_df, all_bird_data, get_transforms('valid'), False)\n",
    "    \n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CONFIG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CONFIG.n_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CONFIG.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG.n_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # == init model ==\n",
    "    bird_model = BirdModel()\n",
    "    \n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_score',\n",
    "                                          dirpath=CONFIG.checkpoint_dir,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='max')\n",
    "    callbacks_to_use = [checkpoint_callback, TQDMProgressBar(refresh_rate=1)]\n",
    "    \n",
    "    # == init trainer ==\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CONFIG.epochs,\n",
    "        val_check_interval=0.5,\n",
    "        callbacks=callbacks_to_use,\n",
    "        enable_model_summary=False,\n",
    "        accelerator=\"gpu\",\n",
    "        deterministic=True,\n",
    "        precision='16-mixed' if CONFIG.mix_precision else 32,\n",
    "    )\n",
    "    \n",
    "    # == Training ==\n",
    "    trainer.fit(bird_model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    # == Prediction ==\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    weights = torch.load(best_model_path)['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "    \n",
    "    preds, gts = predict(val_dl, bird_model)\n",
    "    \n",
    "    # = create dataframe =\n",
    "    pred_df = pd.DataFrame(preds, columns=label_list)\n",
    "    pred_df['id'] = np.arange(len(pred_df))\n",
    "    gt_df = pd.DataFrame(gts, columns=label_list)\n",
    "    gt_df['id'] = np.arange(len(gt_df))\n",
    "    \n",
    "    # = compute score =\n",
    "    val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "    \n",
    "    # == save to file ==\n",
    "    pred_cols = [f'pred_{t}' for t in label_list]\n",
    "    valid_df = pd.concat([valid_df.reset_index(), pd.DataFrame(np.zeros((len(valid_df), len(label_list)*2)).astype(np.float32), columns=label_list+pred_cols)], axis=1)\n",
    "    valid_df[label_list] = gts\n",
    "    valid_df[pred_cols] = preds\n",
    "    valid_df.to_csv(f\"{CONFIG.checkpoint_dir}/pred_df_f{fold_id}.csv\", index=False)\n",
    "    \n",
    "    return preds, gts, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_holdout(f, holdout_df):\n",
    "    bird_model = BirdModel()\n",
    "    weights = torch.load(os.path.join(CONFIG.checkpoint_dir, 'fold_' + str(f) + '.ckpt'), map_location=torch.device('cpu'))['state_dict']\n",
    "    bird_model.load_state_dict(weights)    \n",
    "    test_dataset = BirdDataset(holdout_df, all_bird_data_holdout, get_transforms('test'), True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG.n_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    preds, gts = predict(test_loader, bird_model)\n",
    "    pred_df = pd.DataFrame(preds, columns=label_list)\n",
    "    pred_df['id'] = np.arange(len(pred_df))\n",
    "    gt_df = pd.DataFrame(gts, columns=label_list)\n",
    "    gt_df['id'] = np.arange(len(gt_df))\n",
    "    holdout_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "    pred_cols = [f'pred_{t}' for t in label_list]\n",
    "    holdout_df = pd.concat([holdout_df.reset_index(), pd.DataFrame(np.zeros((len(holdout_df), len(label_list)*2)).astype(np.float32), columns=label_list+pred_cols)], axis=1)\n",
    "    holdout_df[label_list] = gts\n",
    "    holdout_df[pred_cols] = preds\n",
    "    holdout_df.to_csv(f\"{CONFIG.checkpoint_dir}/holdout_pred_df_f{f}.csv\", index=False)\n",
    "    return preds, gts, holdout_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "==== Running training for fold 0 ====\n",
      "Train Samples: 16131\n",
      "Valid Samples: 8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m val_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m f]\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# main loop of f-fold\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m val_preds, val_gts, val_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m oof_df\u001b[38;5;241m.\u001b[39mloc[val_idx, label_list] \u001b[38;5;241m=\u001b[39m val_gts\n\u001b[1;32m     33\u001b[0m oof_df\u001b[38;5;241m.\u001b[39mloc[val_idx, pred_cols] \u001b[38;5;241m=\u001b[39m val_preds\n",
      "Cell \u001b[0;32mIn[72], line 58\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(fold_id, total_df)\u001b[0m\n\u001b[1;32m     47\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     48\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m     49\u001b[0m     val_check_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m16-mixed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m CONFIG\u001b[38;5;241m.\u001b[39mmix_precision \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# == Training ==\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbird_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# == Prediction ==\u001b[39;00m\n\u001b[1;32m     61\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1034\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/evaluation_loop.py:109\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@_no_grad_context\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_OUT_DICT]:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/evaluation_loop.py:191\u001b[0m, in \u001b[0;36m_EvaluationLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_batches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dl \u001b[38;5;129;01min\u001b[39;00m combined_loader\u001b[38;5;241m.\u001b[39mflattened:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# determine number of batches\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dl) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_len_all_ranks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_zero_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    192\u001b[0m     limit_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(trainer, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;241m.\u001b[39mdataloader_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m     num_batches \u001b[38;5;241m=\u001b[39m _parse_num_batches(stage, length, limit_batches)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/data.py:103\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, strategy, allow_zero_length_dataloader_with_multiple_devices)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    102\u001b[0m total_length \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mreduce(torch\u001b[38;5;241m.\u001b[39mtensor(local_length, device\u001b[38;5;241m=\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mroot_device), reduce_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtotal_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[1;32m    104\u001b[0m     rank_zero_warn(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal length of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataloader)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` across ranks is zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please make sure this was your intention.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m local_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "\n",
    "# training\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# record\n",
    "fold_val_score_list = list()\n",
    "oof_df = train_df.copy()\n",
    "pred_cols = [f'pred_{t}' for t in label_list]\n",
    "oof_df = pd.concat([oof_df, pd.DataFrame(np.zeros((len(oof_df), len(pred_cols)*2)).astype(np.float32), columns=label_list+pred_cols)], axis=1)\n",
    "\n",
    "if CONFIG.use_holdout:\n",
    "    # record\n",
    "    fold_test_score_list = list()\n",
    "    holdout_df.index = range(len(holdout_df))\n",
    "    ood_df = holdout_df.copy()\n",
    "    pred_cols = [f'pred_{t}' for t in label_list]\n",
    "    hold_idx = holdout_df.index\n",
    "    ood_df = pd.concat([ood_df, pd.DataFrame(np.zeros((len(ood_df), len(pred_cols)*2)).astype(np.float32), columns=label_list+pred_cols)], axis=1)\n",
    "    hold_predictions = list()\n",
    "\n",
    "for f in range(CONFIG.folds):\n",
    "    \n",
    "    # get validation index\n",
    "    val_idx = list(train_df[train_df['fold'] == f].index)\n",
    "    \n",
    "    # main loop of f-fold\n",
    "    val_preds, val_gts, val_score = run_training(f, train_df)\n",
    "    oof_df.loc[val_idx, label_list] = val_gts\n",
    "    oof_df.loc[val_idx, pred_cols] = val_preds\n",
    "    fold_val_score_list.append(val_score)\n",
    "\n",
    "    if CONFIG.use_holdout:\n",
    "        # Holdout Fold Inference\n",
    "        hold_preds, hold_gts, hold_score = run_holdout(f, holdout_df)\n",
    "        ood_df.loc[hold_idx, label_list] = hold_gts\n",
    "        ood_df.loc[hold_idx, pred_cols] = hold_preds\n",
    "        fold_test_score_list.append(hold_score)\n",
    "        hold_predictions.append(hold_preds)\n",
    "\n",
    "for idx, val_score in enumerate(fold_val_score_list):\n",
    "    print(f'Fold {idx} Val Score: {val_score:.5f}')\n",
    "oof_df.to_csv(f\"{CONFIG.checkpoint_dir}/oof_pred.csv\", index=False)\n",
    "\n",
    "# Holdout Score\n",
    "if CONFIG.use_holdout:\n",
    "\n",
    "    for idx, hold_score in enumerate(fold_test_score_list):\n",
    "        print(f'Fold {idx} Holdout Score: {hold_score:.5f}')\n",
    "    \n",
    "    ood_df.to_csv(f\"{CONFIG.checkpoint_dir}/ood_pred.csv\", index=False)\n",
    "\n",
    "    ensemble_df = holdout_df.copy()\n",
    "    pred_cols = [f'pred_{t}' for t in label_list]\n",
    "    ensemble_df = pd.concat([ensemble_df, pd.DataFrame(np.zeros((len(ensemble_df), len(pred_cols)*2)).astype(np.float32), columns=label_list+pred_cols)], axis=1)\n",
    "    ensemble_predictions = np.mean(hold_predictions, axis=0)\n",
    "    pred_df = pd.DataFrame(ensemble_predictions, columns=label_list)\n",
    "    pred_df['id'] = np.arange(len(pred_df))\n",
    "    gt_df = pd.DataFrame(hold_gts, columns=label_list)\n",
    "    gt_df['id'] = np.arange(len(gt_df))\n",
    "    holdout_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "    print('Ensemble Score', holdout_score)\n",
    "    ensemble_df.loc[hold_idx, label_list] = hold_gts\n",
    "    ensemble_df.loc[hold_idx, pred_cols] = ensemble_predictions\n",
    "    ensemble_df.to_csv(f\"{CONFIG.checkpoint_dir}/holdout_ensemble_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
