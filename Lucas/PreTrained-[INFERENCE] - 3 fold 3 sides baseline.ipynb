{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# BirdCLEF 2024 [Inference]\n",
    "\n",
    "## Features\n",
    "- PyTorch's Dataset & Dataloader\n",
    "- Use PyTorch-Lightning for building model\n",
    "- Data slice is based on @MARK WIJKHUIZEN's [notebook](https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0\n",
    "# !pip install pytorch_lightning==2.1\n",
    "# !pip install pandas librosa opencv-python matplotlib  #cupy-cuda110 \n",
    "# !pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy import signal as sci_signal\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CONFIG:\n",
    "    \n",
    "    # == GENERAL ==\n",
    "    seed = 42                           # random seed\n",
    "    device = 'cpu'                         # device to be used\n",
    "    \n",
    "    # == DATA ==\n",
    "    # preprocessed_data = '../../preprocessed_data/imgs_v0/'                  # Path for processed data to be stores (Must put on .gitignore to not send to repo)\n",
    "    checkpoint_dir = '../chpks/effnet_3fold_3sides_gaussnoise'   # Checkpoints path (Must put on .gitignore to not send to repo)\n",
    "    data_dir_2024 = '../../data/2024'# root folder\n",
    "    sr = 32000                              # sampling rate\n",
    "    n_fft = 1095                            # NFFT of Spec.\n",
    "    win_len = 412                           # WIN_SIZE of Spec.\n",
    "    hop_len = 100                           # overlap of Spec.\n",
    "    min_freq = 40                           # min frequency\n",
    "    max_freq = 15000                        # max frequency\n",
    "    \n",
    "    # == MODEL ==\n",
    "    model = 'efficientnet_b0'               # model architecture\n",
    "    \n",
    "    # == DATASET ==\n",
    "    batch_size = 64                         # batch size of each step\n",
    "    n_workers = 4                           # number of workers\n",
    "\n",
    "print('fix seed')\n",
    "pl.seed_everything(CONFIG.seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "label_list = sorted(os.listdir(os.path.join(CONFIG.data_dir_2024, 'train_audio')))\n",
    "label_id_list = list(range(len(label_list)))\n",
    "label2id = dict(zip(label_list, label_id_list))\n",
    "id2label = dict(zip(label_id_list, label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opposite_melspectrogram(data):\n",
    "    n_fft = CONFIG.n_fft\n",
    "    hop_length = CONFIG.hop_len\n",
    "    rate = CONFIG.sr\n",
    "    n_mels = 128\n",
    "\n",
    "    mel_frequencies = librosa.mel_frequencies(n_mels=128)\n",
    "    \n",
    "    # Inverter as janelas mel para que as frequências mais altas tenham janelas mais curtas\n",
    "    mel_window = librosa.filters.mel(sr= rate, n_fft = n_fft, n_mels=n_mels, htk=True)\n",
    "    \n",
    "    # Inverter a ordem das janelas\n",
    "    mel_window = mel_window[:, ::-1]\n",
    "    \n",
    "    # Calcular o espectrograma mel com a escala mel customizada\n",
    "    S = np.dot(mel_window, np.log(np.abs(librosa.stft(data, n_fft=n_fft, hop_length=hop_length))**2 + 1e-20))\n",
    "\n",
    "    spec_data = librosa.amplitude_to_db(S, ref=np.max)\n",
    "\n",
    "    return spec_data\n",
    "\n",
    "def oog2spec_via_scipy(audio_data):\n",
    "    # handles NaNs\n",
    "    mean_signal = np.nanmean(audio_data)\n",
    "    audio_data = np.nan_to_num(audio_data, nan=mean_signal) if np.isnan(audio_data).mean() < 1 else np.zeros_like(audio_data)\n",
    "    \n",
    "    # to spec.\n",
    "    frequencies, times, spec_data = sci_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=CONFIG.sr, \n",
    "        nfft=CONFIG.n_fft, \n",
    "        nperseg=CONFIG.win_len, \n",
    "        noverlap=CONFIG.hop_len, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    # Filter frequency range\n",
    "    valid_freq = (frequencies >= CONFIG.min_freq) & (frequencies <= CONFIG.max_freq)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # Log\n",
    "    spec_data = np.log10(spec_data + 1e-20)\n",
    "    \n",
    "    # min/max normalize\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Parallel(n_jobs=os.cpu_count())(delayed(lambda x: x)(i) for i in range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bird_data = dict()\n",
    "\n",
    "# # https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference\n",
    "# if len(glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')) > 0:\n",
    "#     ogg_file_paths = glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')\n",
    "# else:\n",
    "#     ogg_file_paths = sorted(glob(f'{CONFIG.data_dir_2024}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "# for i, file_path in tqdm(enumerate(ogg_file_paths)):\n",
    "\n",
    "#     row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)  # filename\n",
    "\n",
    "#     audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "\n",
    "#     for i in range(48):\n",
    "#         input_data = audio_data[5*i*CONFIG.sr:5*(i+1)*CONFIG.sr]\n",
    "#         spec = oog2spec_via_scipy(input_data)\n",
    "#         # print(spec.shape)\n",
    "#         all_bird_data[f'{row_id}_{(i+1)*5}'] = spec\n",
    "#         # break\n",
    "#     # print(R.shape, G.shape, B.shape)\n",
    "    \n",
    "#     # pad\n",
    "#     # pad = 512 - (R.shape[1] % 512)\n",
    "#     # if pad > 0:\n",
    "#     #     R = np.pad(R, ((0,0), (0,pad)))\n",
    "#     #     G = np.pad(G, ((0,0), (0,pad)))\n",
    "#     #     B = np.pad(B, ((0,0), (0,pad)))\n",
    "#     #     # print(spec.shape)\n",
    "#     # # reshape\n",
    "#     # R = R.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "#     # G = G.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "#     # B = B.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "    \n",
    "    \n",
    "#     # # print(spec.shape)\n",
    "#     # # spec = cv2.resize(spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "#     # R = cv2.resize(R, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "#     # G = cv2.resize(G, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "#     # B = cv2.resize(B, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "#     # print(G.shape)\n",
    "#     # # spec = np.array([R,G,B]).transpose(1,2,0) # (256,256,3)\n",
    "\n",
    "#     # break\n",
    "#     # print(spec.shape)\n",
    "#     # for j in range(48):\n",
    "#         # all_bird_data[f'{row_id}_{(j+1)*5}'] = spec[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_specs(file_path, all_bird_data):\n",
    "\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)  # filename\n",
    "\n",
    "    audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "    \n",
    "    # to spec.\n",
    "    spec = oog2spec_via_scipy(audio_data)\n",
    "    \n",
    "    # pad\n",
    "    pad = 512 - (spec.shape[1] % 512)\n",
    "    if pad > 0:\n",
    "        spec = np.pad(spec, ((0,0), (0,pad)))\n",
    "    \n",
    "    # reshape\n",
    "    spec = spec.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "    spec = cv2.resize(spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    for j in range(48):\n",
    "        all_bird_data[f'{row_id}_{(j+1)*5}'] = spec[:, :, j]\n",
    "        \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# all_bird_data_ = dict()\n",
    "\n",
    "# # https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference\n",
    "# if len(glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')) > 0:\n",
    "#     ogg_file_paths = glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')\n",
    "# else:\n",
    "#     ogg_file_paths = sorted(glob(f'{CONFIG.data_dir_2024}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "# for i, file_path in tqdm(enumerate(ogg_file_paths)):\n",
    "#     all_bird_data_ = get_batched_specs(file_path, all_bird_data_)\n",
    "\n",
    "# end = time.time()\n",
    "\n",
    "# print(f\"It took {end-start}s to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 3 ckpts in ../chpks/effnet_3fold_3sides_gaussnoise.\n"
     ]
    }
   ],
   "source": [
    "class EffNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type, n_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if model_type == 'efficientnet_b0':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B0_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b0(weights=weights)\n",
    "        elif model_type == 'efficientnet_b1':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B1_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b1(weights=weights)\n",
    "        elif model_type == 'efficientnet_b2':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B2_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b2(weights=weights)\n",
    "        elif model_type == 'efficientnet_b3':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B3_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b3(weights=weights)\n",
    "        else:\n",
    "            raise ValueError('model type not supported')\n",
    "        \n",
    "        self.base_model.classifier[1] = nn.Linear(self.base_model.classifier[1].in_features, n_classes, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.cat([x, x, x], dim=3).permute(0, 3, 1, 2)\n",
    "        return self.base_model(x)\n",
    "\n",
    "class BirdModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        self.backbone = EffNet(CONFIG.model, n_classes=len(label_list))\n",
    "        \n",
    "        # == loss function ==\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=CONFIG.lr,\n",
    "            weight_decay=CONFIG.weight_decay\n",
    "        )\n",
    "        \n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=CONFIG.epochs,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        \n",
    "        # target to one-hot\n",
    "        target_val = torch.nn.functional.one_hot(target_val, len(label_list))\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "        \n",
    "        self.log(\"val_score\", val_score, True)\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_score': val_score}\n",
    "\n",
    "def predict(spec, models):\n",
    "\n",
    "    spec = torch.tensor(spec, dtype=torch.float32)\n",
    "\n",
    "    pred = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(spec.permute(2,0,1))\n",
    "            outputs = nn.Softmax(dim=1)(outputs)\n",
    "        pred.append(outputs.detach().cpu().numpy())\n",
    "    \n",
    "    # pred = torch.cat(pred, dim=0).cpu().detach()\n",
    "    gc.collect()\n",
    "    # print(outputs.shape, len(pred), len(np.mean(pred, axis =0)))\n",
    "    return np.mean(pred, axis = 0)\n",
    "\n",
    "ckpt_list = glob(f'{CONFIG.checkpoint_dir}/*.ckpt')\n",
    "print(f'find {len(ckpt_list)} ckpts in {CONFIG.checkpoint_dir}.')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "models = []\n",
    "for ckpt in ckpt_list:\n",
    "    \n",
    "    # == init model ==\n",
    "    bird_model = BirdModel()\n",
    "    \n",
    "    # == load ckpt ==\n",
    "    weights = torch.load(ckpt, map_location=torch.device('cpu'))['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "\n",
    "    bird_model.to(CONFIG.device)\n",
    "    bird_model.eval()\n",
    "    models.append(bird_model)\n",
    "    gc.collect()\n",
    "\n",
    "# predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "# sub_pred = pd.DataFrame(predictions, columns=label_list)\n",
    "# sub_id = pd.DataFrame({'row_id': list(all_bird_data.keys())})\n",
    "\n",
    "# sub = pd.concat([sub_id, sub_pred], axis=1)\n",
    "\n",
    "# sub.to_csv('submission.csv',index=False)\n",
    "# print(f'Submissionn shape: {sub.shape}')\n",
    "# sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting file order\n",
    "def get_key_names(file_path):\n",
    "    names = []\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)\n",
    "    for j in range(48):\n",
    "        names.append(f'{row_id}_{(j+1)*5}')\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = predict(all_bird_data_[0]['1001358022_5'], models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_predict(file_path, birds_dict_preds, models):\n",
    "    s_a = time.time()\n",
    "    ## GET BATCHED SPECS\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)  # filename\n",
    "\n",
    "    audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "    \n",
    "    # to spec.\n",
    "    spec = oog2spec_via_scipy(audio_data)\n",
    "    \n",
    "    # pad\n",
    "    pad = 512 - (spec.shape[1] % 512)\n",
    "    if pad > 0:\n",
    "        spec = np.pad(spec, ((0,0), (0,pad)))\n",
    "    \n",
    "    # reshape\n",
    "    spec = spec.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "    spec = cv2.resize(spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    e_a = time.time()\n",
    "    \n",
    "    print(f'It took {e_a - s_a}s to process audio')\n",
    "        \n",
    "    s_m = time.time()\n",
    "    \n",
    "    preds = predict(spec, models)\n",
    "\n",
    "    e_m = time.time()\n",
    "    \n",
    "    print(f'It took {e_m - s_m}s to process model')\n",
    "    # print(preds.shape)\n",
    "    for j in range(48):\n",
    "        # print('starting preds')\n",
    "        birds_dict_preds[f'{row_id}_{(j+1)*5}'] = preds[j]\n",
    "\n",
    "\n",
    "\n",
    "    return birds_dict_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../data/2024/unlabeled_soundscapes/1000170626.ogg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 2.8311386108398438s to process audio\n",
      "It took 2.9410200119018555s to process model\n"
     ]
    }
   ],
   "source": [
    "birds_dict_preds = {}\n",
    "birds_dict_preds = partial_predict(file_path, birds_dict_preds, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissionn shape: (480, 183)\n",
      "It took 27.620320081710815s to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "birds_dict_preds = dict()\n",
    "\n",
    "# https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference\n",
    "if len(glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')) > 0:\n",
    "    ogg_file_paths = glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')\n",
    "else:\n",
    "    ogg_file_paths = sorted(glob(f'{CONFIG.data_dir_2024}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "\n",
    "_convert = partial(\n",
    "    partial_predict,\n",
    "    birds_dict_preds=birds_dict_preds,\n",
    "    models = models\n",
    ")\n",
    "\n",
    "birds_dict_preds = Parallel(n_jobs=4)(delayed(_convert)(file_path) for file_path in tqdm(ogg_file_paths))\n",
    "\n",
    "# for i, file_path in tqdm(enumerate(ogg_file_paths)):\n",
    "    # all_bird_data_ = get_batched_specs(file_path, all_bird_data)\n",
    "\n",
    "indices = []\n",
    "\n",
    "for file_path in ogg_file_paths:\n",
    "    indices.extend(get_key_names(file_path))\n",
    "\n",
    "dicionario_final = {}\n",
    "\n",
    "for dicionario in birds_dict_preds:\n",
    "    dicionario_final.update(dicionario)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for idx in indices:\n",
    "    predictions.append(dicionario_final[idx])\n",
    "\n",
    "\n",
    "sub_pred = pd.DataFrame(predictions, columns=label_list)\n",
    "sub_id = pd.DataFrame({'row_id': indices})\n",
    "\n",
    "sub = pd.concat([sub_id, sub_pred], axis=1)\n",
    "\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "print(f'Submissionn shape: {sub.shape}')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"It took {end-start}s to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Time for submission: ~ 00 hr: 50 min : 38 sec\n"
     ]
    }
   ],
   "source": [
    "sub_time = (end - start) * 110  # Calculate estimated submission time for ~1100 recordings\n",
    "sub_time = time.gmtime(sub_time)  # Convert seconds to a time tuple\n",
    "sub_time = time.strftime(\"%H hr: %M min : %S sec\", sub_time)  # Format time tuple as string\n",
    "print(f\">> Time for submission: ~ {sub_time}\")  # Print estimated submission time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: 43.09616661071777\n",
      ">> Time for submission: ~ 01 hr: 19 min : 00 sec\n",
      "It took 4.376332998275757s to process audio\n",
      "It took 4.394300937652588s to process model\n",
      "It took 2.613518238067627s to process audio\n",
      "It took 4.428742408752441s to process model\n",
      "It took 4.609344720840454s to process audio\n",
      "It took 4.433002233505249s to process model\n",
      "It took 2.044949769973755s to process audio\n",
      "It took 4.765061855316162s to process model\n",
      "It took 2.2476513385772705s to process audio\n",
      "It took 4.422472953796387s to process model\n",
      "It took 1.703033208847046s to process audio\n",
      "It took 3.049412488937378s to process model\n",
      "It took 1.8218040466308594s to process audio\n",
      "It took 4.012712240219116s to process model\n",
      "It took 4.415438890457153s to process audio\n",
      "It took 4.641656160354614s to process model\n",
      "It took 2.5014965534210205s to process audio\n",
      "It took 4.735945701599121s to process model\n",
      "It took 1.8956804275512695s to process audio\n",
      "It took 2.371521234512329s to process model\n"
     ]
    }
   ],
   "source": [
    "e = time.time()\n",
    "\n",
    "print(f'Notebook runtime: {e-s}')\n",
    "\n",
    "sub_time = (e - s) * 110  # Calculate estimated submission time for ~1100 recordings\n",
    "sub_time = time.gmtime(sub_time)  # Convert seconds to a time tuple\n",
    "sub_time = time.strftime(\"%H hr: %M min : %S sec\", sub_time)  # Format time tuple as string\n",
    "print(f\">> Time for submission: ~ {sub_time}\")  # Print estimated submission time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4779991,
     "sourceId": 8120971,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
