{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# BirdCLEF 2024 [Inference]\n",
    "\n",
    "## Features\n",
    "- PyTorch's Dataset & Dataloader\n",
    "- Use PyTorch-Lightning for building model\n",
    "- Data slice is based on @MARK WIJKHUIZEN's [notebook](https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0\n",
    "# !pip install pytorch_lightning==2.1\n",
    "# !pip install pandas librosa opencv-python matplotlib  #cupy-cuda110 \n",
    "# !pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy import signal as sci_signal\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CONFIG:\n",
    "    \n",
    "    # == GENERAL ==\n",
    "    seed = 42                           # random seed\n",
    "    device = 'cpu'                         # device to be used\n",
    "    \n",
    "    # == DATA ==\n",
    "    # preprocessed_data = '../../preprocessed_data/imgs_v0/'                  # Path for processed data to be stores (Must put on .gitignore to not send to repo)\n",
    "    checkpoint_dir = '../chpks/effnet_3fold_rgb'   # Checkpoints path (Must put on .gitignore to not send to repo)\n",
    "    data_dir_2024 = '../../data/2024'# root folder\n",
    "    sr = 32000                              # sampling rate\n",
    "    n_fft = 1095                            # NFFT of Spec.\n",
    "    win_len = 412                           # WIN_SIZE of Spec.\n",
    "    hop_len = 100                           # overlap of Spec.\n",
    "    min_freq = 40                           # min frequency\n",
    "    max_freq = 15000                        # max frequency\n",
    "    n_mels = 128\n",
    "    \n",
    "    # == MODEL ==\n",
    "    model = 'efficientnet_b0'               # model architecture\n",
    "    \n",
    "    # == DATASET ==\n",
    "    batch_size = 64                         # batch size of each step\n",
    "    n_workers = 4                           # number of workers\n",
    "\n",
    "print('fix seed')\n",
    "pl.seed_everything(CONFIG.seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "label_list = sorted(os.listdir(os.path.join(CONFIG.data_dir_2024, 'train_audio')))\n",
    "label_id_list = list(range(len(label_list)))\n",
    "label2id = dict(zip(label_list, label_id_list))\n",
    "id2label = dict(zip(label_id_list, label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opposite_melspectrogram(data):\n",
    "    n_fft = CONFIG.n_fft\n",
    "    hop_length = CONFIG.hop_len\n",
    "    rate = CONFIG.sr\n",
    "    n_mels = 128\n",
    "\n",
    "    mel_frequencies = librosa.mel_frequencies(n_mels=128)\n",
    "    \n",
    "    # Inverter as janelas mel para que as frequências mais altas tenham janelas mais curtas\n",
    "    mel_window = librosa.filters.mel(sr= rate, n_fft = n_fft, n_mels=n_mels, htk=True)\n",
    "    \n",
    "    # Inverter a ordem das janelas\n",
    "    mel_window = mel_window[:, ::-1]\n",
    "    \n",
    "    # Calcular o espectrograma mel com a escala mel customizada\n",
    "    S = np.dot(mel_window, np.log(np.abs(librosa.stft(data, n_fft=n_fft, hop_length=hop_length))**2 + 1e-20))\n",
    "\n",
    "    spec_data = librosa.amplitude_to_db(S, ref=np.max)\n",
    "\n",
    "    return spec_data\n",
    "\n",
    "def oog2spec_via_scipy(audio_data):\n",
    "    # HANDLE NaNs\n",
    "    mean_signal = np.nanmean(audio_data)\n",
    "    input_audio = np.nan_to_num(audio_data, nan=mean_signal) if np.isnan(audio_data).mean() < 1 else np.zeros_like(input_audio)\n",
    "    \n",
    "    # SPECTROGRAM\n",
    "    frequencies, times, spec_data = sci_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=CONFIG.sr, \n",
    "        nfft=CONFIG.n_fft, \n",
    "        nperseg=CONFIG.win_len, \n",
    "        noverlap=CONFIG.hop_len, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Inverter as janelas mel para que as frequências mais altas tenham janelas mais curtas\n",
    "    mel_window = librosa.filters.mel(sr= CONFIG.sr, n_fft = CONFIG.n_fft, n_mels=CONFIG.n_mels, htk=True, fmin= CONFIG.min_freq, fmax = CONFIG.max_freq)\n",
    "    \n",
    "    # Calcular o espectrograma mel com a escala mel customizada\n",
    "    melspec = np.dot(mel_window, np.log10(np.abs(spec_data) + 1e-20))\n",
    "    # melspec = librosa.amplitude_to_db(melspec, ref=np.max)\n",
    "    \n",
    "    # Inverter a ordem das janelas\n",
    "    mel_window = mel_window[:, ::-1]\n",
    "    \n",
    "    # Calcular o espectrograma mel com a escala mel customizada\n",
    "    oposite_melspec = np.dot(mel_window, np.log10(np.abs(spec_data) + 1e-20))\n",
    "    # oposite_melspec = librosa.amplitude_to_db(oposite_melspec, ref=np.max)\n",
    "    \n",
    "    # FILTER LOWER AND HIGHER FREQUENCIES\n",
    "    valid_freq = (frequencies >= CONFIG.min_freq) & (frequencies <= CONFIG.max_freq)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # COMPUTE LOG SPEC\n",
    "    spec_data = np.log10(spec_data + 1e-20)\n",
    "\n",
    "    \n",
    "    # MIN/MAX NORMALIZATION\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "\n",
    "    # MIN/MAX NORMALIZATION\n",
    "    melspec = melspec - melspec.min()\n",
    "    melspec = melspec / melspec.max()\n",
    "\n",
    "    # MIN/MAX NORMALIZATION\n",
    "    oposite_melspec = oposite_melspec - oposite_melspec.min()\n",
    "    oposite_melspec = oposite_melspec / oposite_melspec.max()\n",
    "    # SPEC TO IMAGE\n",
    "    # R = cv2.resize(R, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    # G = cv2.resize(G, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    # B = cv2.resize(B, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # return np.array([R,G,B]).transpose(1,2,0) # (256,256,3)\n",
    "    return spec_data, melspec, oposite_melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../data/2024/unlabeled_soundscapes/1000170626.ogg'\n",
    "audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "# HANDLE NaNs\n",
    "spec_data, melspec, oposite_melspec = oog2spec_via_scipy(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 24615), 0.0, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melspec.shape, melspec.min(), melspec.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 24615), 0.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oposite_melspec.shape, oposite_melspec.min(), oposite_melspec.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 24615), 0.0, 1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_data.shape, spec_data.min(), spec_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Parallel(n_jobs=os.cpu_count())(delayed(lambda x: x)(i) for i in range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bird_data = dict()\n",
    "\n",
    "# # https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference\n",
    "# if len(glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')) > 0:\n",
    "#     ogg_file_paths = glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')\n",
    "# else:\n",
    "#     ogg_file_paths = sorted(glob(f'{CONFIG.data_dir_2024}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "# for i, file_path in tqdm(enumerate(ogg_file_paths)):\n",
    "\n",
    "#     row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)  # filename\n",
    "\n",
    "#     audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "\n",
    "#     for i in range(48):\n",
    "#         input_data = audio_data[5*i*CONFIG.sr:5*(i+1)*CONFIG.sr]\n",
    "#         spec = oog2spec_via_scipy(input_data)\n",
    "#         # print(spec.shape)\n",
    "#         all_bird_data[f'{row_id}_{(i+1)*5}'] = spec\n",
    "#         # break\n",
    "#     # print(R.shape, G.shape, B.shape)\n",
    "    \n",
    "#     # pad\n",
    "#     # pad = 512 - (R.shape[1] % 512)\n",
    "#     # if pad > 0:\n",
    "#     #     R = np.pad(R, ((0,0), (0,pad)))\n",
    "#     #     G = np.pad(G, ((0,0), (0,pad)))\n",
    "#     #     B = np.pad(B, ((0,0), (0,pad)))\n",
    "#     #     # print(spec.shape)\n",
    "#     # # reshape\n",
    "#     # R = R.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "#     # G = G.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "#     # B = B.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "    \n",
    "    \n",
    "#     # # print(spec.shape)\n",
    "#     # # spec = cv2.resize(spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "#     # R = cv2.resize(R, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "#     # G = cv2.resize(G, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "#     # B = cv2.resize(B, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "#     # print(G.shape)\n",
    "#     # # spec = np.array([R,G,B]).transpose(1,2,0) # (256,256,3)\n",
    "\n",
    "#     # break\n",
    "#     # print(spec.shape)\n",
    "#     # for j in range(48):\n",
    "#         # all_bird_data[f'{row_id}_{(j+1)*5}'] = spec[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_specs(file_path):\n",
    "\n",
    "    audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1) \n",
    "\n",
    "    R, G ,B = oog2spec_via_scipy(audio_data)\n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    # pad\n",
    "    pad = 512 - (R.shape[1] % 512)\n",
    "    if pad > 0:\n",
    "        R = np.pad(R, ((0,0), (0,pad)))\n",
    "    pad = 1600 - (G.shape[1] % 1600)\n",
    "    if pad > 0:\n",
    "        G = np.pad(G, ((0,0), (0,pad)))\n",
    "        B = np.pad(B, ((0,0), (0,pad)))\n",
    "    #     # print(spec.shape)\n",
    "\n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    # reshape\n",
    "    R = R.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "    G = G.reshape(128,-1,1600).transpose([0, 2, 1])\n",
    "    B = B.reshape(128,-1,1600).transpose([0, 2, 1])\n",
    "    \n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    # print(spec.shape)\n",
    "    # spec = cv2.resize(spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    R = cv2.resize(R, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    G = cv2.resize(G, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    B = cv2.resize(B, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    spec = np.array([R,G,B]).transpose(1,2,0, 3) # (256,256,3)\n",
    "\n",
    "\n",
    "    for j in range(48):\n",
    "        all_bird_data[f'{row_id}_{(j+1)*5}'] = spec[:, :, :, j]\n",
    "    \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# all_bird_data_ = dict()\n",
    "\n",
    "# # https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference\n",
    "# if len(glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')) > 0:\n",
    "#     ogg_file_paths = glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')\n",
    "# else:\n",
    "#     ogg_file_paths = sorted(glob(f'{CONFIG.data_dir_2024}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "# for i, file_path in tqdm(enumerate(ogg_file_paths)):\n",
    "#     all_bird_data_ = get_batched_specs(file_path, all_bird_data_)\n",
    "\n",
    "# end = time.time()\n",
    "\n",
    "# print(f\"It took {end-start}s to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 3 ckpts in ../chpks/effnet_3fold_rgb.\n"
     ]
    }
   ],
   "source": [
    "class EffNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type, n_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if model_type == 'efficientnet_b0':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B0_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b0(weights=weights)\n",
    "        elif model_type == 'efficientnet_b1':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B1_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b1(weights=weights)\n",
    "        elif model_type == 'efficientnet_b2':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B2_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b2(weights=weights)\n",
    "        elif model_type == 'efficientnet_b3':\n",
    "            if pretrained: weights = efficientnet.EfficientNet_B3_Weights.DEFAULT\n",
    "            else: weights = None\n",
    "            self.base_model = efficientnet.efficientnet_b3(weights=weights)\n",
    "        else:\n",
    "            raise ValueError('model type not supported')\n",
    "        \n",
    "        self.base_model.classifier[1] = nn.Linear(self.base_model.classifier[1].in_features, n_classes, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,3,1,2)\n",
    "        return self.base_model(x)\n",
    "\n",
    "class BirdModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        self.backbone = EffNet(CONFIG.model, n_classes=len(label_list))\n",
    "        \n",
    "        # == loss function ==\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=CONFIG.lr,\n",
    "            weight_decay=CONFIG.weight_decay\n",
    "        )\n",
    "        \n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=CONFIG.epochs,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        \n",
    "        # target to one-hot\n",
    "        target_val = torch.nn.functional.one_hot(target_val, len(label_list))\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "        \n",
    "        self.log(\"val_score\", val_score, True)\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_score': val_score}\n",
    "\n",
    "def predict(spec, models):\n",
    "\n",
    "    spec = torch.tensor(spec, dtype=torch.float32)\n",
    "\n",
    "    pred = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(spec.permute(3,0,1,2))\n",
    "            outputs = nn.Softmax(dim=1)(outputs)\n",
    "        pred.append(outputs.detach().cpu().numpy())\n",
    "    \n",
    "    # pred = torch.cat(pred, dim=0).cpu().detach()\n",
    "    gc.collect()\n",
    "    # print(outputs.shape, len(pred), len(np.mean(pred, axis =0)))\n",
    "    return np.mean(pred, axis = 0)\n",
    "\n",
    "ckpt_list = glob(f'{CONFIG.checkpoint_dir}/*.ckpt')\n",
    "print(f'find {len(ckpt_list)} ckpts in {CONFIG.checkpoint_dir}.')\n",
    "ckpt_list = [ckpt_list[1]]\n",
    "predictions = []\n",
    "\n",
    "models = []\n",
    "for ckpt in ckpt_list:\n",
    "    \n",
    "    # == init model ==\n",
    "    bird_model = BirdModel()\n",
    "    \n",
    "    # == load ckpt ==\n",
    "    weights = torch.load(ckpt, map_location=torch.device('cpu'))['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "\n",
    "    bird_model.to(CONFIG.device)\n",
    "    bird_model.eval()\n",
    "    models.append(bird_model)\n",
    "    gc.collect()\n",
    "\n",
    "# predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "# sub_pred = pd.DataFrame(predictions, columns=label_list)\n",
    "# sub_id = pd.DataFrame({'row_id': list(all_bird_data.keys())})\n",
    "\n",
    "# sub = pd.concat([sub_id, sub_pred], axis=1)\n",
    "\n",
    "# sub.to_csv('submission.csv',index=False)\n",
    "# print(f'Submissionn shape: {sub.shape}')\n",
    "# sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = predict(all_bird_data_[0]['1001358022_5'], models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_predict(file_path, birds_dict_preds, models):\n",
    "\n",
    "    ## GET BATCHED SPECS\n",
    "    audio_data, _ = librosa.load(file_path, sr=CONFIG.sr)\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1) \n",
    "\n",
    "    R, G ,B = oog2spec_via_scipy(audio_data)\n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    # pad\n",
    "    pad = 512 - (R.shape[1] % 512)\n",
    "    if pad > 0:\n",
    "        R = np.pad(R, ((0,0), (0,pad)))\n",
    "    # pad = 512 - (G.shape[1] % 1600)\n",
    "    # if pad > 0:\n",
    "        G = np.pad(G, ((0,0), (0,pad)))\n",
    "        B = np.pad(B, ((0,0), (0,pad)))\n",
    "    #     # print(spec.shape)\n",
    "\n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    # reshape\n",
    "    R = R.reshape(512,-1,512).transpose([0, 2, 1])\n",
    "    G = G.reshape(128,-1,512).transpose([0, 2, 1])\n",
    "    B = B.reshape(128,-1,512).transpose([0, 2, 1])\n",
    "    \n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    # print(spec.shape)\n",
    "    # spec = cv2.resize(spec, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    R = cv2.resize(R, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    G = cv2.resize(G, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    B = cv2.resize(B, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # print(R)\n",
    "    # print(R.shape, G.shape, B.shape)\n",
    "    spec = np.array([R,G,B]).transpose(1,2,0,3) # (256,256,3)\n",
    "\n",
    "    preds = predict(spec, models)\n",
    "\n",
    "    # print(preds.shape)\n",
    "    for j in range(48):\n",
    "        # print('starting preds')\n",
    "        birds_dict_preds[f'{row_id}_{(j+1)*5}'] = preds[j]\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return birds_dict_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/2024/unlabeled_soundscapes/1000170626.ogg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_dict_preds = {}\n",
    "birds_dict_preds = partial_predict(file_path, birds_dict_preds, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting file order\n",
    "def get_key_names(file_path):\n",
    "    names = []\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)\n",
    "    for j in range(48):\n",
    "        names.append(f'{row_id}_{(j+1)*5}')\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_key_names(file_path)[2]\n",
    "\n",
    "\n",
    "if len(glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')) > 0:\n",
    "    ogg_file_paths = glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')\n",
    "else:\n",
    "    ogg_file_paths = sorted(glob(f'{CONFIG.data_dir_2024}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "indices = []\n",
    "\n",
    "for file_path in ogg_file_paths:\n",
    "    indices.extend(get_key_names(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Função que será executada em paralelo\n",
    "def minha_funcao(arg):\n",
    "    time.sleep(1)  # Simula uma operação demorada\n",
    "    return arg * 2\n",
    "\n",
    "# Lista de argumentos para a função\n",
    "argumentos = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Criar um ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Mapear a função em cada argumento\n",
    "    resultados = list(executor.map(minha_funcao, argumentos))\n",
    "\n",
    "    # Aguardar até que todas as tarefas pendentes sejam concluídas\n",
    "    executor.shutdown(wait=True)\n",
    "\n",
    "print(\"Resultados:\", resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m _convert \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m     13\u001b[0m     partial_predict,\n\u001b[1;32m     14\u001b[0m     birds_dict_preds\u001b[38;5;241m=\u001b[39mbirds_dict_preds,\n\u001b[1;32m     15\u001b[0m     models \u001b[38;5;241m=\u001b[39m models\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print('iniciando paralel')\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m birds_dict_preds \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mogg_file_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print('acabou paralel')\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# for i, file_path in tqdm(enumerate(ogg_file_paths)):\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# all_bird_data_ = get_batched_specs(file_path, all_bird_data)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m indices \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/site-packages/joblib/parallel.py:1909\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend:\n\u001b[0;32m-> 1909\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1911\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_effective_n_jobs()\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/site-packages/joblib/parallel.py:1359\u001b[0m, in \u001b[0;36mParallel._initialize_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a process or thread pool and return the number of workers\"\"\"\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1359\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39msupports_timeout:\n\u001b[1;32m   1362\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1363\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe backend class \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m does not support timeout. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1364\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in Parallel but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter will not be used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/site-packages/joblib/_parallel_backends.py:538\u001b[0m, in \u001b[0;36mLokyBackend.configure\u001b[0;34m(self, n_jobs, parallel, prefer, require, idle_worker_timeout, **memmappingexecutor_args)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FallbackToBackend(\n\u001b[1;32m    536\u001b[0m         SequentialBackend(nesting_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnesting_level))\n\u001b[0;32m--> 538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[43mget_memmapping_executor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midle_worker_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_worker_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmemmappingexecutor_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel \u001b[38;5;241m=\u001b[39m parallel\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_jobs\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/site-packages/joblib/executor.py:20\u001b[0m, in \u001b[0;36mget_memmapping_executor\u001b[0;34m(n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_memmapping_executor\u001b[39m(n_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMemmappingExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_memmapping_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/site-packages/joblib/executor.py:52\u001b[0m, in \u001b[0;36mMemmappingExecutor.get_memmapping_executor\u001b[0;34m(cls, n_jobs, timeout, initializer, initargs, env, temp_folder, context_id, **backend_args)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# reducers access the temporary folder in which to store temporary\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# pickles through a call to manager.resolve_temp_folder_name. resolving\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# the folder name dynamically is useful to use different folders across\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# calls of a same reusable executor\u001b[39;00m\n\u001b[1;32m     48\u001b[0m job_reducers, result_reducers \u001b[38;5;241m=\u001b[39m get_memmapping_reducers(\n\u001b[1;32m     49\u001b[0m     unlink_on_gc_collect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     temp_folder_resolver\u001b[38;5;241m=\u001b[39mmanager\u001b[38;5;241m.\u001b[39mresolve_temp_folder_name,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbackend_args)\n\u001b[0;32m---> 52\u001b[0m _executor, executor_is_reused \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reusable_executor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_reducers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_reducers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_reducers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_reducers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m executor_is_reused:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Only set a _temp_folder_manager for new executors. Reused\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# executors already have a _temporary_folder_manager that must not\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# be re-assigned like that because it is referenced in various\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# places in the reducing machinery of the executor.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     _executor\u001b[38;5;241m.\u001b[39m_temp_folder_manager \u001b[38;5;241m=\u001b[39m manager\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/site-packages/joblib/externals/loky/reusable_executor.py:207\u001b[0m, in \u001b[0;36m_ReusablePoolExecutor.get_reusable_executor\u001b[0;34m(cls, max_workers, context, timeout, kill_workers, reuse, job_reducers, result_reducers, initializer, initargs, env)\u001b[0m\n\u001b[1;32m    201\u001b[0m     reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments have changed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m mp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating a new executor with max_workers=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_workers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as the previous instance cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreused (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m )\n\u001b[0;32m--> 207\u001b[0m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m _executor \u001b[38;5;241m=\u001b[39m executor \u001b[38;5;241m=\u001b[39m _executor_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Recursive call to build a new instance\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:1303\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;66;03m# is shutting down.\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _global_shutdown_lock:\n\u001b[0;32m-> 1303\u001b[0m         \u001b[43mexecutor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m         _threads_wakeups\u001b[38;5;241m.\u001b[39mpop(executor_manager_thread, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "start = time.time()\n",
    "birds_dict_preds = dict()\n",
    "\n",
    "if len(glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')) > 0:\n",
    "    ogg_file_paths = glob(f'{CONFIG.data_dir_2024}/test_soundscapes/*.ogg')\n",
    "else:\n",
    "    ogg_file_paths = sorted(glob(f'{CONFIG.data_dir_2024}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "\n",
    "_convert = partial(\n",
    "    partial_predict,\n",
    "    birds_dict_preds=birds_dict_preds,\n",
    "    models = models\n",
    ")\n",
    "\n",
    "\n",
    "# print('iniciando paralel')\n",
    "birds_dict_preds = Parallel(n_jobs=2, timeout=10)(delayed(_convert)(file_path) for file_path in tqdm(ogg_file_paths))\n",
    "# print('acabou paralel')\n",
    "# for i, file_path in tqdm(enumerate(ogg_file_paths)):\n",
    "    # all_bird_data_ = get_batched_specs(file_path, all_bird_data)\n",
    "\n",
    "indices = []\n",
    "\n",
    "for file_path in ogg_file_paths:\n",
    "    indices.extend(get_key_names(file_path))\n",
    "\n",
    "dicionario_final = {}\n",
    "\n",
    "for dicionario in birds_dict_preds:\n",
    "    dicionario_final.update(dicionario)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for idx in indices:\n",
    "    predictions.append(dicionario_final[idx])\n",
    "\n",
    "\n",
    "sub_pred = pd.DataFrame(predictions, columns=label_list)\n",
    "sub_id = pd.DataFrame({'row_id': indices})\n",
    "\n",
    "sub = pd.concat([sub_id, sub_pred], axis=1)\n",
    "\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "print(f'Submissionn shape: {sub.shape}')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"It took {end-start}s to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_time = (end - start) *  110 # Calculate estimated submission time for ~1100 recordings\n",
    "sub_time = time.gmtime(sub_time)  # Convert seconds to a time tuple\n",
    "sub_time = time.strftime(\"%H hr: %M min : %S sec\", sub_time)  # Format time tuple as string\n",
    "print(f\">> Time for submission: ~ {sub_time}\")  # Print estimated submission time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ogg_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = time.time()\n",
    "\n",
    "print(f'Notebook runtime: {e-s}')\n",
    "\n",
    "sub_time = (e - s) * 110  # Calculate estimated submission time for ~1100 recordings\n",
    "sub_time = time.gmtime(sub_time)  # Convert seconds to a time tuple\n",
    "sub_time = time.strftime(\"%H hr: %M min : %S sec\", sub_time)  # Format time tuple as string\n",
    "print(f\">> Time for submission: ~ {sub_time}\")  # Print estimated submission time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "datasetId": 4779991,
     "sourceId": 8120971,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
