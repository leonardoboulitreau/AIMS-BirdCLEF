{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ed6536-6d39-47d9-925d-a2b08914ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install pytorch_lightning==2.1\n",
    "# !pip install pandas librosa opencv-python matplotlib  #cupy-cuda110 \n",
    "# !pip install -U albumentations\n",
    "# import timm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a953c0c0-f2d7-45ad-a19f-dc44d170331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal as sci_signal\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import albumentations as albu\n",
    "from torchvision.models import efficientnet\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "mel_spec_params = {\n",
    "    \"sample_rate\": 32000,\n",
    "    \"n_mels\": 128,\n",
    "    \"f_min\": 20,\n",
    "    \"f_max\": 16000,\n",
    "    \"n_fft\": 2048,\n",
    "    \"hop_length\": 512,\n",
    "    \"normalized\": True,\n",
    "    \"center\" : True,\n",
    "    \"pad_mode\" : \"constant\",\n",
    "    \"norm\" : \"slaney\",\n",
    "    \"onesided\" : True,\n",
    "    \"mel_scale\" : \"slaney\"\n",
    "}\n",
    "def normalize_melspec(X, eps=1e-6):\n",
    "    mean = X.mean((1, 2), keepdim=True)\n",
    "    std = X.std((1, 2), keepdim=True)\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "\n",
    "    norm_min, norm_max = (\n",
    "        Xstd.min(-1)[0].min(-1)[0],\n",
    "        Xstd.max(-1)[0].max(-1)[0],\n",
    "    )\n",
    "    fix_ind = (norm_max - norm_min) > eps * torch.ones_like(\n",
    "        (norm_max - norm_min)\n",
    "    )\n",
    "    V = torch.zeros_like(Xstd)\n",
    "    if fix_ind.sum():\n",
    "        V_fix = Xstd[fix_ind]\n",
    "        norm_max_fix = norm_max[fix_ind, None, None]\n",
    "        norm_min_fix = norm_min[fix_ind, None, None]\n",
    "        V_fix = torch.max(\n",
    "            torch.min(V_fix, norm_max_fix),\n",
    "            norm_min_fix,\n",
    "        )\n",
    "        V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "        V[fix_ind] = V_fix\n",
    "    return V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f128f0-dac5-4a88-a45d-89a5b2874fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    '''\n",
    "    Version of macro-averaged ROC-AUC score that ignores all classes that have no true positive labels.\n",
    "    '''\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    if not pandas.api.types.is_numeric_dtype(submission.values):\n",
    "        bad_dtypes = {x: submission[x].dtype  for x in submission.columns if not pandas.api.types.is_numeric_dtype(submission[x])}\n",
    "        raise ParticipantVisibleError(f'Invalid submission data types found: {bad_dtypes}')\n",
    "\n",
    "    solution_sums = solution.sum(axis=0)\n",
    "    scored_columns = list(solution_sums[solution_sums > 0].index.values)\n",
    "    assert len(scored_columns) > 0\n",
    "\n",
    "    return sklearn.metrics.roc_auc_score(solution[scored_columns].values, submission[scored_columns].values, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5330c965-1837-4ed3-bcf9-babb21248edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    \n",
    "    # == GENERAL ==\n",
    "    seed = 42                             # random seed\n",
    "    device = 'cuda'                         # device to be used\n",
    "    mix_precision = False                   # whether to use mixed-16 precision\n",
    "    \n",
    "    # == DATA ==\n",
    "    preprocess = True\n",
    "    preprocessed_data = '../../preprocessed_data/imgs_v0/'                  # Path for processed data to be stores (Must put on .gitignore to not send to repo)\n",
    "    checkpoint_dir = '../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_finetuning_5epocas'  # Checkpoints path (Must put on .gitignore to not send to repo)\n",
    "    data_dir_2024 = '../../data/2024'# root folder\n",
    "    sr = 32000                              # sampling rate\n",
    "    n_fft = 1095                            # NFFT of Spec.\n",
    "    win_len = 412                          # WIN_SIZE of Spec.\n",
    "    hop_len = 100                         # overlap of Spec.\n",
    "    min_freq = 40                           # min frequency\n",
    "    max_freq = 15000                        # max frequency\n",
    "    \n",
    "    # == MODEL ==\n",
    "    model = 'efficientnet_b0'               # model architecture\n",
    "    \n",
    "    # == DATASET ==\n",
    "    batch_size = 32                         # batch size of each step\n",
    "    n_workers = 4                           # number of workers\n",
    "    \n",
    "    # == AUG ==\n",
    "    USE_HORIZFLIP = True\n",
    "    USE_XYMASKING = True                    # whether use XYMasking\n",
    "\n",
    "    # == TRAINING ==\n",
    "    folds = 5                              # n fold\n",
    "    epochs = 5                             # max epochs\n",
    "    epochs_stage_1 = 5\n",
    "    epochs_stage_2 = 10\n",
    "    lr = 3e-4                               # learning rate\n",
    "    weight_decay = 1e-6                     # weight decay of optimizer\n",
    "    visualize = True                        # whether to visualize data and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ee1e70-a248-48c6-b4c3-3379c7e10990",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'no_dups_30sec.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_dups_30sec.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Labels\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'no_dups_30sec.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"no_dups_30sec.csv\")\n",
    "df.head()\n",
    "\n",
    "# Labels\n",
    "label_list = sorted(df['primary_label'].unique())\n",
    "label_id_list = list(range(len(label_list)))\n",
    "label2id = dict(zip(label_list, label_id_list))\n",
    "id2label = dict(zip(label_id_list, label_list))\n",
    "\n",
    "train_df = df[['primary_label', 'wav_path','secondary_labels','rating']].copy()  # Uses only audio, label and rating.\n",
    "\n",
    "# Create Target\n",
    "train_df['target'] = train_df.primary_label.map(label2id)\n",
    "\n",
    "# Create Filepath\n",
    "# train_df['filepath'] = CONFIG.data_dir_2024 + '/train_audio/' + train_df.filename\n",
    "\n",
    "# Create Name\n",
    "train_df['name'] = train_df.wav_path.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e11dc4-aa5a-4a1f-9417-1ef096643c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "def preprocess_fn(filepath, input_audio = None):\n",
    "\n",
    "    if(input_audio is not None):\n",
    "        waveform = input_audio\n",
    "    else:\n",
    "    # LOAD .OGG\n",
    "        waveform, sample_rate = torchaudio.load(filepath, normalize=True)\n",
    "        # waveform = waveform[:5*sample_rate]\n",
    "\n",
    "    # print(len(waveform.shape))\n",
    "    # if(len(waveform.shape) < 2):\n",
    "        # waveform\n",
    "    \n",
    "    transform = torchaudio.transforms.MelSpectrogram(**mel_spec_params)\n",
    "    spectrogram = transform(waveform)\n",
    "    db_transform = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80)\n",
    "    spectrogram = db_transform(spectrogram)\n",
    "\n",
    "    # print(spectrogram.shape)\n",
    "    spec_data = normalize_melspec(spectrogram).squeeze(0) \n",
    "    # spec_data = spec_data.expand(3, -1, -1).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # print(spec_data.shape)\n",
    "    # spec_data = albu.Resize(256, 256)(image=spec_data)\n",
    "    # spec_data = albu.Normalize()(image=spec_data['image'])['image'].astype(np.float32)\n",
    "    # spec = res['image'].astype(np.float32)\n",
    "    # spec_data = spec_data.transpose(2, 0, 1)\n",
    "    # print(spec_data['image'].shape)\n",
    "    return spec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ff194b-7a81-4d62-8124-22a452304f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Just testing\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m processed_spec_left \u001b[38;5;241m=\u001b[39m preprocess_fn(\u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m processed_spec_center \u001b[38;5;241m=\u001b[39m preprocess_fn(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m processed_spec_right \u001b[38;5;241m=\u001b[39m preprocess_fn(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Just testing\n",
    "\n",
    "processed_spec_left = preprocess_fn(train_df['wav_path'].values[0])\n",
    "processed_spec_center = preprocess_fn(train_df['wav_path'].values[1])\n",
    "processed_spec_right = preprocess_fn(train_df['wav_path'].values[2])\n",
    "\n",
    "plt.figure()\n",
    "plt.title('LEFT')\n",
    "plt.imshow(processed_spec_left, origin='lower')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('CENTER')\n",
    "plt.imshow(processed_spec_center, origin='lower')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"RIGHT\")\n",
    "plt.imshow(processed_spec_right, origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a22bca-7e39-4f71-b766-3881292aa8ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_spec_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocessed_spec_center\u001b[49m\u001b[38;5;241m.\u001b[39mshape, processed_spec_center\u001b[38;5;241m.\u001b[39mmean(), processed_spec_center\u001b[38;5;241m.\u001b[39mmin()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed_spec_center' is not defined"
     ]
    }
   ],
   "source": [
    "processed_spec_center.shape, processed_spec_center.mean(), processed_spec_center.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbb7c33-3050-4c54-ad7d-26dfa44cdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torchaudio\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158093c6-afd3-44e4-8134-63bff1657696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class BackgroundAugmentation(object):\n",
    "    def __init__(self, min_scale, max_scale, back_df, mode = 'train'):\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.backgrounds = back_df\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        numero_aleatorio = random.randint(0, len(self.backgrounds)-1)\n",
    "        noise_path = self.backgrounds.wav_path.values[numero_aleatorio]\n",
    "        noise, sample_rate = librosa.load(noise_path, sr = None)\n",
    "        # noise = np.concatenate([noise]*3) #if training with 15sec\n",
    "            \n",
    "        rand_scale = random.uniform(self.min_scale, self.max_scale)\n",
    "#         print(rand_scale)\n",
    "        noisy_speech = (rand_scale * waveform + noise) / (1+rand_scale)\n",
    "#         print(waveform.mean(), noisy_speech.mean())\n",
    "        return noisy_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c195cc-95ad-404e-ab8e-c77bed15e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_augs = os.listdir('../../preprocessed_data/5sec_unlabeleds_center/')\n",
    "list_augs = ['../../preprocessed_data/5sec_unlabeleds_center/' + w for w in list_augs]\n",
    "\n",
    "ex_df = pd.DataFrame({'wav_path': list_augs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa42225f-0893-488f-83fa-fc02cfc4380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backaud = BackgroundAugmentation(0.1,0.1, ex_df, mode = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe95d1cc-4673-45e2-a231-8ebe59794188",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\u001b[43mex\u001b[49m, sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m waveform \u001b[38;5;241m=\u001b[39m backaud(waveform[:\u001b[38;5;241m32000\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m      3\u001b[0m Audio(waveform, rate \u001b[38;5;241m=\u001b[39m sample_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ex' is not defined"
     ]
    }
   ],
   "source": [
    "waveform, sample_rate = librosa.load(ex, sr = None)\n",
    "waveform = backaud(waveform[:32000*5])\n",
    "Audio(waveform, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097423b-ff0f-4818-ae6d-63b48cdc3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df_train, ex_df_val = train_test_split(ex_df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7919e5a-1b6f-4198-bd65-a2fd5ca94725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfmixup(data, alpha):\n",
    "    \n",
    "    data = torch.tensor(data)\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    # shuffled_targets = targets[indices]\n",
    "    #print(indices)\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    # print(lam)\n",
    "    #print(lam)\n",
    "    new_data = data * lam + shuffled_data * (1 - lam)\n",
    "    # new_targets = targets * lam + shuffled_targets * (1 - lam)\n",
    "    return new_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485a5d2d-9671-4d00-9b40-548748d6dc95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y , sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mwav_path[\u001b[38;5;241m0\u001b[39m], sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(sr, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m wave \u001b[38;5;241m=\u001b[39m selfmixup(data\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32000\u001b[39m), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "y , sr = librosa.load(train_df.wav_path[0], sr = None)\n",
    "print(sr, y.shape)\n",
    "wave = selfmixup(data=y.reshape(-1, 5*32000), alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e50d6a-9e63-4a47-aa27-dab426cb724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class SumMixUp(object):\n",
    "    def __init__(self, df, labels, min_pct= 0.3, max_pct=1):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.min_pct = min_pct\n",
    "        self.max_pct = max_pct\n",
    "\n",
    "    def __call__(self, waveform, label):\n",
    "        idx = random.randint(0, len(self.df)-1)\n",
    "        noise_path = self.df.wav_path.values[idx]\n",
    "        random_wav, sample_rate = librosa.load(noise_path, sr = None)\n",
    "        random_label = self.labels[idx]\n",
    "\n",
    "        rand1 = random.uniform(0, self.max_pct - self.min_pct) + self.min_pct\n",
    "        rand2 = random.uniform(0, self.max_pct - self.min_pct) + self.min_pct\n",
    "\n",
    "\n",
    "        ridx = random.randint(0, 5) # Works because of 6 * 5sec chunks\n",
    "        waveform = rand1*waveform + rand2*random_wav[ridx*32000:(ridx+5)*32000]\n",
    "        \n",
    "        if rand1 >= 0.5:\n",
    "            rand1 = 1\n",
    "        else:\n",
    "            rand1 = 1 - 2*(0.5 - rand1)\n",
    "\n",
    "        if rand2 >= 0.5:\n",
    "            rand2 = 1\n",
    "        else:\n",
    "            rand2 = 1 - 2*(0.5 - rand2)\n",
    "\n",
    "        # print(rand1, rand2)\n",
    "        \n",
    "        label = rand1*label + rand2*random_label\n",
    "\n",
    "        # print(label)\n",
    "        return waveform , np.clip(label, 0 ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0007c81-c8fb-48bf-a1bd-799fa6b72506",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m182\u001b[39m)[\u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "labels = np.eye(182)[train_df[\"target\"].astype(int).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f799b9b7-834c-4e1f-9dab-f79eb465b4cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m smu \u001b[38;5;241m=\u001b[39m SumMixUp(\u001b[43mtrain_df\u001b[49m, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "smu = SumMixUp(train_df, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1acc9da2-cacc-4bc4-9f14-50f24b713fec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m y , sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mwav_path[i], sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sr, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m wave, l \u001b[38;5;241m=\u001b[39m smu(y[:\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32000\u001b[39m], labels[i])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "y , sr = librosa.load(train_df.wav_path[i], sr = None)\n",
    "print(sr, y.shape)\n",
    "wave, l = smu(y[:5*32000], labels[i])\n",
    "Audio(wave, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0917e78f-fcf4-4579-8467-7f5bca020108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        back_augmentation = None,\n",
    "        use_sumixup = False,\n",
    "        spec_augmentation=None,\n",
    "        mode='train'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.back_augmentation = back_augmentation\n",
    "        self.summixup = None\n",
    "        self.spec_augmentation = spec_augmentation\n",
    "        self.mode = mode\n",
    "        self.labels = np.eye(self.df.target.nunique())[self.df[\"target\"].astype(int).values]\n",
    "\n",
    "        if use_sumixup:\n",
    "            print(\"Using SumMixUp\")\n",
    "            self.summixup = SumMixUp(self.df, self.labels)\n",
    "\n",
    "        if self.back_augmentation is not None:\n",
    "            print(\"Using random background noise of unlabelled soundscapes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # def __getitem__(self, index):\n",
    "    def __getitem__(self, index):\n",
    "        wav_path = self.df.wav_path.values[index]\n",
    "        # target\n",
    "        target = self.labels[index]\n",
    "        \n",
    "        input_audio, sr = torchaudio.load(wav_path, normalize=True)\n",
    "        input_audio = input_audio[0].numpy()\n",
    "\n",
    "        rand_scale = random.uniform(0, 1)\n",
    "        ridx = random.randint(0, 5)\n",
    "        if(rand_scale > 0.2):\n",
    "            wave = selfmixup(data=input_audio.reshape(-1, 5*32000), alpha=0.95)\n",
    "            input_audio = wave[ridx:ridx+1, :][0]\n",
    "        else:\n",
    "            input_audio = input_audio[ridx*32000:(ridx+5)*32000]\n",
    "\n",
    "        if self.summixup is not None:\n",
    "            # print('using sumixup')\n",
    "            input_audio, target = self.summixup(input_audio, target)\n",
    "            \n",
    "        # aug\n",
    "        if self.back_augmentation is not None:\n",
    "            # print('using backaug')\n",
    "            rand_scale = random.uniform(0, 1)\n",
    "            if(rand_scale > 0.3 and self.mode == 'train'):\n",
    "                input_audio = self.back_augmentation(input_audio)\n",
    "            if(self.mode == 'valid'):\n",
    "                input_audio = self.back_augmentation(input_audio)\n",
    "                \n",
    "        input_spec = preprocess_fn('', input_audio = torch.tensor(input_audio).unsqueeze(0))\n",
    "        \n",
    "        if self.spec_augmentation is not None:\n",
    "            input_spec = self.spec_augmentation(image=input_spec.numpy())['image']\n",
    "        \n",
    "        return torch.tensor(input_spec, dtype=torch.float32) ,torch.tensor(target.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8699f1e7-b4b0-453e-9706-3e06b659a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(_type):\n",
    "    \n",
    "    if _type == 'train':\n",
    "        return albu.Compose([\n",
    "            albu.GaussNoise(var_limit=(0.01, 0.05), mean=0),\n",
    "            albu.HorizontalFlip(0.5) if CONFIG.USE_XYMASKING else albu.NoOp(),\n",
    "            albu.XYMasking(\n",
    "                p=0.3,\n",
    "                num_masks_x=(1, 5),\n",
    "                num_masks_y=(1, 3),\n",
    "                mask_x_length=(1, 20),\n",
    "                mask_y_length=(1, 5),\n",
    "            ) if CONFIG.USE_XYMASKING else albu.NoOp()\n",
    "        ])\n",
    "    elif _type == 'valid':\n",
    "        return albu.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55ac8d21-9a22-48e9-90fc-6ca51f2a37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(ds, row=3, col=3):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    img_index = np.random.randint(0, len(ds)-1, row*col)\n",
    "    \n",
    "    for i in range(len(img_index)):\n",
    "        img, label = dummy_dataset[img_index[i]]\n",
    "        \n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # print(img.min().item())\n",
    "            min_ = img.min().item()\n",
    "            max_ = img.max().item()\n",
    "            # print(min_, max_)\n",
    "            # print(img_index[i], img)\n",
    "            img = img.detach().numpy()\n",
    "            # print(img.min(), img.max())\n",
    "        \n",
    "        ax = fig.add_subplot(row, col, i + 1, xticks=[], yticks=[])\n",
    "        # ax.imshow(img, cmap='jet')\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'ID: {img_index[i]}; Target: {label.argmax()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "119d08de-df6d-47a0-913c-e9858807deaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m backaud \u001b[38;5;241m=\u001b[39m BackgroundAugmentation(\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.1\u001b[39m, ex_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m dummy_dataset \u001b[38;5;241m=\u001b[39m BirdDataset( \n\u001b[0;32m----> 4\u001b[0m         \u001b[43mtrain_df\u001b[49m,\n\u001b[1;32m      5\u001b[0m         back_augmentation \u001b[38;5;241m=\u001b[39m backaud,\n\u001b[1;32m      6\u001b[0m         use_sumixup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m         spec_augmentation\u001b[38;5;241m=\u001b[39mget_transforms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m test_input, test_target \u001b[38;5;241m=\u001b[39m dummy_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_input\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "backaud = BackgroundAugmentation(0.1,0.1, ex_df, 'train')\n",
    "\n",
    "dummy_dataset = BirdDataset( \n",
    "        train_df,\n",
    "        back_augmentation = backaud,\n",
    "        use_sumixup = True,\n",
    "        spec_augmentation=get_transforms('train'),\n",
    "        mode='train')\n",
    "\n",
    "test_input, test_target = dummy_dataset[0]\n",
    "print(test_input.detach().numpy().shape)\n",
    "\n",
    "show_batch(dummy_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a8dd574-a3a2-47c5-ae97-8e6e37563eae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dummy_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dummy_dataset\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dummy_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "del dummy_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "843f2693-8a4d-4a6c-b030-e7955e3f1985",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Getting original splits\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mfolds, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf\u001b[38;5;241m.\u001b[39msplit(train_df, y\u001b[38;5;241m=\u001b[39mtrain_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m val_idx:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Getting original splits\n",
    "kf = StratifiedKFold(n_splits=CONFIG.folds, shuffle=True, random_state=CONFIG.seed)\n",
    "train_df['fold'] = 0\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, y=train_df['target'])):\n",
    "    for idx in val_idx:\n",
    "        train_df.iloc[idx, train_df.columns.get_loc('fold') ] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1879d04-d65c-4965-b173-080411daaeab",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2022_2023_30sec.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reading additional data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m aug_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2022_2023_30sec.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m aug_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2022_2023_30sec.csv'"
     ]
    }
   ],
   "source": [
    "# Reading additional data\n",
    "aug_df = pd.read_csv(\"2022_2023_30sec.csv\")\n",
    "aug_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da0ffa4a-bd35-42aa-b492-e53f9ec1e6cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aug_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aug_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaped\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43maug_df\u001b[49m\u001b[38;5;241m.\u001b[39mprimary_label\u001b[38;5;241m.\u001b[39mmap(aug_df\u001b[38;5;241m.\u001b[39mprimary_label\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[1;32m      2\u001b[0m aug_df \u001b[38;5;241m=\u001b[39m aug_df[(aug_df\u001b[38;5;241m.\u001b[39mmaped \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m (aug_df\u001b[38;5;241m.\u001b[39mmaped \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m)]\n\u001b[1;32m      3\u001b[0m aug_df\u001b[38;5;241m.\u001b[39mprimary_label\u001b[38;5;241m.\u001b[39mnunique()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aug_df' is not defined"
     ]
    }
   ],
   "source": [
    "aug_df['maped'] = aug_df.primary_label.map(aug_df.primary_label.value_counts())\n",
    "aug_df = aug_df[(aug_df.maped > 10) & (aug_df.maped < 500)]\n",
    "aug_df.primary_label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38cee7e1-8fc1-4f6c-85b7-432bf6510f2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aug_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # Labels\u001b[39;00m\n\u001b[1;32m      2\u001b[0m init_new_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m182\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43maug_df\u001b[49m\u001b[38;5;241m.\u001b[39mprimary_label\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m label2id\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      5\u001b[0m         label2id[p] \u001b[38;5;241m=\u001b[39m init_new_label\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aug_df' is not defined"
     ]
    }
   ],
   "source": [
    "# # Labels\n",
    "init_new_label = 182\n",
    "for p in aug_df.primary_label.unique():\n",
    "    if p not in label2id.keys():\n",
    "        label2id[p] = init_new_label\n",
    "        init_new_label+= 1\n",
    "        \n",
    "id2label = dict(zip(label_id_list, label_list))\n",
    "\n",
    "aug_df = aug_df[['primary_label', 'wav_path','secondary_labels','rating']].copy()  # Uses only audio, label and rating.\n",
    "\n",
    "# # Create Target\n",
    "aug_df['target'] = aug_df.primary_label.map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1656571-9a93-4515-be4a-e699d41e28f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aug_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maug_df\u001b[49m\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aug_df' is not defined"
     ]
    }
   ],
   "source": [
    "aug_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cdacff0-5dba-4172-94c1-2ac720c28ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aug_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Getting augmented splits\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mfolds, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m----> 3\u001b[0m \u001b[43maug_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf\u001b[38;5;241m.\u001b[39msplit(aug_df, y\u001b[38;5;241m=\u001b[39maug_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m val_idx:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aug_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Getting augmented splits\n",
    "kf = StratifiedKFold(n_splits=CONFIG.folds, shuffle=True, random_state=CONFIG.seed)\n",
    "aug_df['fold'] = 0\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(aug_df, y=aug_df['target'])):\n",
    "    for idx in val_idx:\n",
    "        aug_df.iloc[idx, aug_df.columns.get_loc('fold') ] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f1b2010-4d53-4750-8c52-a6fa45a001aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mtrain_df\u001b[49m, aug_df])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m train_df\u001b[38;5;241m.\u001b[39mfold\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train_df, aug_df]).reset_index(drop=True)\n",
    "train_df.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8411bfc-d02a-45ec-8549-79227331ff45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mfold\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m fold]\u001b[38;5;241m.\u001b[39mprimary_label\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "for fold in train_df.fold.unique():\n",
    "    print(train_df[train_df['fold']== fold].primary_label.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be683786-9a02-471c-9013-5e24424b97fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_label_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(full_label_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "full_label_list = sorted(train_df['primary_label'].unique())\n",
    "len(full_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a34f5779-5529-4479-935c-8eee5e7297b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Beta\n",
    "class Mixup(nn.Module):\n",
    "    def __init__(self, mix_beta):\n",
    "\n",
    "        super(Mixup, self).__init__()\n",
    "        self.beta_distribution = Beta(mix_beta, mix_beta)\n",
    "\n",
    "    def forward(self, X, Y, weight=None):\n",
    "\n",
    "        bs = X.shape[0]\n",
    "        n_dims = len(X.shape)\n",
    "        perm = torch.randperm(bs)\n",
    "        coeffs = self.beta_distribution.rsample(torch.Size((bs,))).to(X.device)\n",
    "\n",
    "        if n_dims == 2:\n",
    "            X = coeffs.view(-1, 1) * X + (1 - coeffs.view(-1, 1)) * X[perm]\n",
    "        elif n_dims == 3:\n",
    "            X = coeffs.view(-1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1)) * X[perm]\n",
    "        else:\n",
    "            X = coeffs.view(-1, 1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm]\n",
    "\n",
    "        Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm]\n",
    "\n",
    "#         print(coeffs, perm)\n",
    "        if weight is None:\n",
    "            return X, Y\n",
    "        else:\n",
    "            weight = coeffs.view(-1) * weight + (1 - coeffs.view(-1)) * weight[perm]\n",
    "            return X, Y, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41da2e74-1d0d-42a8-95e1-45f90da0e024",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msoup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetaStyleEncoder\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEffNet\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_classes):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'soup'"
     ]
    }
   ],
   "source": [
    "from soup import MetaStyleEncoder\n",
    "class EffNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = efficientnet.efficientnet_b0(weights=efficientnet.EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "        \n",
    "        self.base_model.classifier[1] = nn.Linear(self.base_model.classifier[1].in_features, n_classes, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = x.unsqueeze(-1)\n",
    "        # x = torch.cat([x, x, x], dim=3).permute(0, 3, 1, 2)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.expand(-1, 3, -1, -1)\n",
    "        return self.base_model(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4207ccff-3780-4649-b9cc-bc5f07c34ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = EffNet(310)\n",
    "        # cpt = torch.load('chpks/effnet_pretrain_crossentropy_pastcompetiions_lr1e3_cosinelr_T200_evalacc1_4MM_emb128_noarcface/best_model_3440.pth')\n",
    "        # self.backbone.load_state_dict(cpt)\n",
    "\n",
    "        # print(self.backbone.base_model.classifier[1])\n",
    "        self.backbone.base_model.classifier[1] = nn.Linear(1280, 490)\n",
    "\n",
    "    def forward(self, mel):\n",
    "        return self.backbone(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd827232-bacb-4566-87d2-e25b610470c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EffNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[43mMyModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m313\u001b[39m))\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m335\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m, in \u001b[0;36mMyModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[43mEffNet\u001b[49m(\u001b[38;5;241m310\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# cpt = torch.load('chpks/effnet_pretrain_crossentropy_pastcompetiions_lr1e3_cosinelr_T200_evalacc1_4MM_emb128_noarcface/best_model_3440.pth')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# self.backbone.load_state_dict(cpt)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(self.backbone.base_model.classifier[1])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1280\u001b[39m, \u001b[38;5;241m490\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EffNet' is not defined"
     ]
    }
   ],
   "source": [
    "n = MyModel()\n",
    "\n",
    "x = torch.rand((2,128,313))\n",
    "y = torch.randint(low=0, high=335, size=(1,1))[0]\n",
    "\n",
    "pred = n(x)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ba06229-ae86-4112-b44e-5346402530db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m n \n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "del n \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9a2692c-84cd-402a-a61c-41652e06d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 0.1,\n",
    "                 min_lr : float = 0.001,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle = 0 # cycle count\n",
    "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # set learning rate min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "class BCEFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n",
    "        probas = torch.sigmoid(preds)\n",
    "\n",
    "        \n",
    "\n",
    "        tmp = targets * self.alpha * (1. - probas)**self.gamma * bce_loss\n",
    "        smp = (1. - targets) * probas**self.gamma * bce_loss\n",
    "        \n",
    "        loss = tmp + smp\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BirdModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,steps_per_epoch):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        # self.backbone = EffNet(CONFIG.model, n_classes=len(full_label_list))\n",
    "        \n",
    "        # self.backbone = MetaStyleEncoder(128,128,182)\n",
    "        self.backbone = MyModel()\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        # == loss function ==\n",
    "#         self.loss_fn = nn.CrossEntropyLoss()\n",
    "        # self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        self.loss_fn = BCEFocalLoss()\n",
    "    \n",
    "        self.mixup = Mixup(mix_beta=.5)\n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=CONFIG.lr,\n",
    "            weight_decay=CONFIG.weight_decay\n",
    "        )\n",
    "\n",
    "        # model_optimizer = Adan(self.parameters(), lr=CONFIG.lr, betas=(0.02, 0.08, 0.01), weight_decay=1e-2)\n",
    "        \n",
    "        # == define learning rate scheduler ==\n",
    "        # lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "        #     model_optimizer,\n",
    "        #     T_0=CONFIG.epochs,\n",
    "        #     T_mult=1,\n",
    "        #     eta_min=1e-6,\n",
    "        #     last_epoch=-1\n",
    "        # )\n",
    "\n",
    "        lr_scheduler = CosineAnnealingWarmupRestarts(\n",
    "            model_optimizer,\n",
    "            first_cycle_steps= CONFIG.epochs,\n",
    "            cycle_mult=CONFIG.epochs,\n",
    "            max_lr= CONFIG.lr,\n",
    "            min_lr=1e-6,\n",
    "            warmup_steps=0,\n",
    "            gamma=1,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "                \n",
    "        # lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        #     optimizer=model_optimizer, epochs=CONFIG.epochs,\n",
    "        #     pct_start=0.0, steps_per_epoch=self.steps_per_epoch,\n",
    "        #     max_lr=CONFIG.lr, div_factor=25, final_div_factor=4.0e-01\n",
    "        # )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "#         num_classes = 182\n",
    "\n",
    "        # rand_scale = random.uniform(0, 1)\n",
    "        # if(rand_scale > 0.5):\n",
    "            # image, target = self.mixup(image, target)\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach().float()\n",
    "        \n",
    "        # target to one-hot\n",
    "#         target_val = torch.nn.functional.one_hot(target_val, len(label_list)).float()\n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val.to(\"cuda\"), target_val.to(\"cuda\"))\n",
    "        \n",
    "        \n",
    "        target_val = np.ceil(target_val)\n",
    "        # = val with ROC AUC =\n",
    "        gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=full_label_list)\n",
    "        pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=full_label_list)\n",
    "        \n",
    "        gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "        \n",
    "        self.log(\"val_score\", val_score, True)\n",
    "        \n",
    "        # clear validation outputs\n",
    "        self.validation_step_outputs = list()\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_score': val_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c43e5ae3-350c-4ef9-ab9c-208109617ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EffNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, \u001b[43mMyModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparameters()),\n\u001b[1;32m      3\u001b[0m     lr\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mlr,\n\u001b[1;32m      4\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mweight_decay\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model_optimizer = Adan(self.parameters(), lr=CONFIG.lr, betas=(0.02, 0.08, 0.01), weight_decay=1e-2)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# == define learning rate scheduler ==\u001b[39;00m\n\u001b[1;32m     10\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m CosineAnnealingWarmupRestarts(\n\u001b[1;32m     11\u001b[0m     model_optimizer,\n\u001b[1;32m     12\u001b[0m     first_cycle_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     last_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m, in \u001b[0;36mMyModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[43mEffNet\u001b[49m(\u001b[38;5;241m310\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# cpt = torch.load('chpks/effnet_pretrain_crossentropy_pastcompetiions_lr1e3_cosinelr_T200_evalacc1_4MM_emb128_noarcface/best_model_3440.pth')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# self.backbone.load_state_dict(cpt)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(self.backbone.base_model.classifier[1])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1280\u001b[39m, \u001b[38;5;241m490\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EffNet' is not defined"
     ]
    }
   ],
   "source": [
    "model_optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, MyModel().parameters()),\n",
    "    lr=CONFIG.lr,\n",
    "    weight_decay=CONFIG.weight_decay\n",
    ")\n",
    "\n",
    "# model_optimizer = Adan(self.parameters(), lr=CONFIG.lr, betas=(0.02, 0.08, 0.01), weight_decay=1e-2)\n",
    "\n",
    "# == define learning rate scheduler ==\n",
    "lr_scheduler = CosineAnnealingWarmupRestarts(\n",
    "    model_optimizer,\n",
    "    first_cycle_steps=30,\n",
    "    cycle_mult=30,\n",
    "    max_lr= CONFIG.lr,\n",
    "    min_lr=1e-7,\n",
    "    warmup_steps=3,\n",
    "    gamma=1,\n",
    "    last_epoch=-1\n",
    ")\n",
    "\n",
    "lrs = []\n",
    "for i in range(30):\n",
    "    lrs.append(lr_scheduler.get_lr())\n",
    "    lr_scheduler.step()\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "493f46fd-08b2-4bb1-9703-89ba23e43fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    model.to(CONFIG.device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch\n",
    "            x = x.cuda()\n",
    "            outputs = model(x)\n",
    "            outputs = nn.Softmax(dim=1)(outputs)\n",
    "        predictions.append(outputs.detach().cpu())\n",
    "        gts.append(y.detach().cpu())\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0).cpu().detach()\n",
    "    gts = torch.cat(gts, dim=0).cpu().detach()\n",
    "#     gts = torch.nn.functional.one_hot(gts, len(label_list))\n",
    "    return predictions.numpy().astype(np.float32), gts.numpy().astype(np.float32)\n",
    "\n",
    "def predict_noised(val_df, model):\n",
    "    backaud_valid = BackgroundAugmentation(0.1,0.1, ex_df_val, mode = 'eval')\n",
    "    val_ds = BirdDataset(val_df, backaud_valid, False, get_transforms('valid'), 'valid')\n",
    "    \n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CONFIG.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG.n_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    mixup = Mixup(mix_beta=1)\n",
    "    model.to(CONFIG.device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for batch in tqdm(val_dl):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch\n",
    "            x, y = mixup(x, y)\n",
    "            x = x.cuda()\n",
    "            outputs = model(x)\n",
    "            outputs = nn.Softmax(dim=1)(outputs)\n",
    "        predictions.append(outputs.detach().cpu())\n",
    "        gts.append(y.detach().cpu())\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0).cpu().detach()\n",
    "    gts = torch.cat(gts, dim=0).cpu().detach()\n",
    "#     gts = torch.nn.functional.one_hot(gts, len(label_list))\n",
    "    return predictions.numpy().astype(np.float32), gts.numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b35521d-261d-454b-a86b-1496c0a3bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, total_df):\n",
    "    print('================================================================')\n",
    "    print(f\"==== Running training for fold {fold_id} ====\")\n",
    "    \n",
    "    # == create dataset and dataloader ==\n",
    "    train_df = total_df[total_df[f'fold'] != fold_id].copy()\n",
    "    valid_df = total_df[total_df[f'fold'] == fold_id].copy()\n",
    "\n",
    "    # train_df = train_df[(train_df['rating'] >= 3)|(train_df['volumetry'] < 20)]\n",
    "    print(f'There are {train_df.primary_label.nunique()} targets')\n",
    "\n",
    "    \n",
    "    print(f'Train Samples: {len(train_df)}')\n",
    "    print(f'Valid Samples: {len(valid_df)}')\n",
    "\n",
    "    # == create a sampler that samples inversely of its counts ==\n",
    "    y_train = train_df['target']\n",
    "#     class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "#     weight = 1. / class_sample_count\n",
    "#     samples_weight = np.array([weight[t] for t in y_train])\n",
    "#     samples_weight = torch.from_numpy(samples_weight)\n",
    "    \n",
    "#     sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "    \n",
    "    \n",
    "    backaud = BackgroundAugmentation(0.1,0.3, ex_df_train)\n",
    "    # backaud = None\n",
    "#     backaud_valid = None\n",
    "#     backaud_valid = BackgroundAugmentation(0.1,0.1, ex_df)\n",
    "    \n",
    "    # train_ds = BirdDataset(train_df, None, True, get_transforms('train'), 'train')\n",
    "    # train_ds = BirdDataset(train_df, backaud, True, None, 'train')\n",
    "    # val_ds = BirdDataset(valid_df, None, False, get_transforms('valid'), 'valid')\n",
    "    train_ds = BirdDataset(train_df, None, False, None, 'train')\n",
    "    val_ds = BirdDataset(valid_df, None, False, None, 'valid')\n",
    "    \n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CONFIG.batch_size,\n",
    "        shuffle=True, # If using sampler must be False\n",
    "        num_workers=CONFIG.n_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "#         sampler = sampler\n",
    "    )\n",
    "    \n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CONFIG.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG.n_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # == init model ==\n",
    "    bird_model = BirdModel(steps_per_epoch = len(train_dl))\n",
    "    # bird_model = BirdModel()\n",
    "\n",
    "    # Uncomment if fine tuning\n",
    "    print('loading pre trained')\n",
    "    cpt = torch.load(f'../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_30epocas/fold_{fold_id}.ckpt', map_location=torch.device('cpu'))['state_dict']\n",
    "    bird_model.load_state_dict(cpt)\n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_score',\n",
    "                                          dirpath=CONFIG.checkpoint_dir,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='max')\n",
    "    callbacks_to_use = [checkpoint_callback, TQDMProgressBar(refresh_rate=1)]\n",
    "    \n",
    "    # == init trainer ==\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CONFIG.epochs,\n",
    "        val_check_interval=0.5,\n",
    "        callbacks=callbacks_to_use,\n",
    "        enable_model_summary=False,\n",
    "        accelerator=\"gpu\",\n",
    "        deterministic=False, # Default is True, but it doest not work with metastylespeech\n",
    "        precision='16-mixed' if CONFIG.mix_precision else 32,\n",
    "    )\n",
    "    \n",
    "    # == Training ==\n",
    "    trainer.fit(bird_model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    # == Prediction ==\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    weights = torch.load(best_model_path)['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "    \n",
    "    preds, gts = predict(val_dl, bird_model)\n",
    "    \n",
    "    # = create dataframe =\n",
    "    pred_df = pd.DataFrame(preds, columns=full_label_list)\n",
    "    pred_df = pred_df[label_list]\n",
    "    pred_df['id'] = np.arange(len(pred_df))\n",
    "    gt_df = pd.DataFrame(gts, columns=full_label_list)\n",
    "    gt_df = gt_df[label_list]\n",
    "    gt_df['id'] = np.arange(len(gt_df))\n",
    "    \n",
    "    # = compute score =\n",
    "    val_score = score(gt_df.apply(np.ceil), pred_df, row_id_column_name='id')\n",
    "    \n",
    "    preds_noised, gts_noised = predict_noised(valid_df, bird_model)\n",
    "    \n",
    "    # = create dataframe =\n",
    "    pred_df_noised = pd.DataFrame(preds_noised, columns=full_label_list)\n",
    "    pred_df_noised = pred_df_noised[label_list]\n",
    "    pred_df_noised['id'] = np.arange(len(pred_df_noised))\n",
    "    gt_df_noised = pd.DataFrame(gts_noised, columns=full_label_list)\n",
    "    gt_df_noised = gt_df_noised[label_list]\n",
    "    gt_df_noised['id'] = np.arange(len(gt_df_noised))\n",
    "    \n",
    "    # = compute score =\n",
    "    val_score_noised = score(gt_df_noised.apply(np.ceil), pred_df_noised, row_id_column_name='id')\n",
    "    \n",
    "    # == save to file ==\n",
    "    pred_cols = [f'pred_{t}' for t in full_label_list]\n",
    "    valid_df = pd.concat([valid_df.reset_index(), pd.DataFrame(np.zeros((len(valid_df), len(full_label_list)*2)).astype(np.float32), columns=full_label_list+pred_cols)], axis=1)\n",
    "    valid_df[full_label_list] = gts\n",
    "    valid_df[pred_cols] = preds\n",
    "    valid_df.to_csv(f\"{CONFIG.checkpoint_dir}/pred_df_f{fold_id}.csv\", index=False)\n",
    "    \n",
    "    return preds, gts, val_score, val_score_noised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b6d07ec-70c8-4152-b80b-2dbcfea35fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m fold_val_score_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     10\u001b[0m fold_val_score_noised_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m---> 11\u001b[0m oof_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     12\u001b[0m pred_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m full_label_list]\n\u001b[1;32m     13\u001b[0m oof_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([oof_df, pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(oof_df), \u001b[38;5;28mlen\u001b[39m(pred_cols)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), columns\u001b[38;5;241m=\u001b[39mfull_label_list\u001b[38;5;241m+\u001b[39mpred_cols)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# training\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "seed = CONFIG.seed\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# record\n",
    "fold_val_score_list = list()\n",
    "fold_val_score_noised_list = list()\n",
    "oof_df = train_df.copy()\n",
    "pred_cols = [f'pred_{t}' for t in full_label_list]\n",
    "oof_df = pd.concat([oof_df, pd.DataFrame(np.zeros((len(oof_df), len(pred_cols)*2)).astype(np.float32), columns=full_label_list+pred_cols)], axis=1)\n",
    "\n",
    "for f in range(CONFIG.folds):\n",
    "    \n",
    "    # get validation index\n",
    "    val_idx = list(train_df[train_df[f'fold'] == f].index)\n",
    "    \n",
    "    # main loop of f-fold\n",
    "    val_preds, val_gts, val_score, val_score_noised = run_training(f, train_df)\n",
    "    \n",
    "    # record\n",
    "    oof_df.loc[val_idx, full_label_list] = val_gts\n",
    "    oof_df.loc[val_idx, pred_cols] = val_preds\n",
    "    fold_val_score_list.append(val_score)\n",
    "    fold_val_score_noised_list.append(val_score_noised)\n",
    "    print(f'Fold {f} Val Score: {fold_val_score_list[f]:.5f} Val Score Noised: {fold_val_score_noised_list[f]:.5f}')\n",
    "    \n",
    "for idx, val_score in enumerate(fold_val_score_list):\n",
    "    print(f'Fold {idx} Val Score: {val_score:.5f} Val Score Noised: {fold_val_score_noised_list[idx]:.5f}')\n",
    "\n",
    "# oof_gt_df = oof_df[['samplename'] + label_list].copy()\n",
    "# oof_pred_df = oof_df[['samplename'] + pred_cols].copy()\n",
    "# oof_pred_df.columns = ['samplename'] + label_list\n",
    "# oof_score = score(oof_gt_df, oof_pred_df, 'samplename')\n",
    "# print(f'OOF Score: {oof_score:.5f}')\n",
    "\n",
    "oof_df.to_csv(f\"{CONFIG.checkpoint_dir}/oof_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c825d135-901a-4938-a7ea-95630abb412f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CONFIG\u001b[38;5;241m.\u001b[39mfolds):\n\u001b[0;32m----> 9\u001b[0m     eval_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m[train_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m f]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     11\u001b[0m     val_ds \u001b[38;5;241m=\u001b[39m BirdDataset(eval_df, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, get_transforms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     val_dl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     14\u001b[0m         val_ds,\n\u001b[1;32m     15\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m         persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "## replicating metrics of actual model but with the same pipeline as previous\n",
    "PATH_TO = CONFIG.checkpoint_dir\n",
    "\n",
    "seed = CONFIG.seed\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for f in range(CONFIG.folds):\n",
    "    eval_df = train_df[train_df[f'fold'] == f].copy()\n",
    "\n",
    "    val_ds = BirdDataset(eval_df, None, False, get_transforms('valid'),'valid')\n",
    "    \n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CONFIG.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG.n_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    bird_model = BirdModel(12)\n",
    "    best_model_path = PATH_TO + f'/fold_{f}.ckpt'\n",
    "    weights = torch.load(best_model_path)['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "\n",
    "    preds, gts = predict(val_dl, bird_model)\n",
    "    \n",
    "    # = create dataframe =\n",
    "    pred_df = pd.DataFrame(preds, columns=full_label_list)\n",
    "    pred_df = pred_df[label_list]\n",
    "    pred_df['id'] = np.arange(len(pred_df))\n",
    "    gt_df = pd.DataFrame(gts, columns=full_label_list)\n",
    "    gt_df = gt_df[label_list]\n",
    "    gt_df['id'] = np.arange(len(gt_df))\n",
    "    \n",
    "    # = compute score =\n",
    "    val_score = score(gt_df.apply(np.ceil), pred_df, row_id_column_name='id')\n",
    "    \n",
    "    preds_noised, gts_noised = predict_noised(eval_df, bird_model)\n",
    "    \n",
    "    # = create dataframe =\n",
    "    # = create dataframe =\n",
    "    pred_df_noised = pd.DataFrame(preds_noised, columns=full_label_list)\n",
    "    pred_df_noised = pred_df_noised[label_list]\n",
    "    pred_df_noised['id'] = np.arange(len(pred_df_noised))\n",
    "    gt_df_noised = pd.DataFrame(gts_noised, columns=full_label_list)\n",
    "    gt_df_noised = gt_df_noised[label_list]\n",
    "    gt_df_noised['id'] = np.arange(len(gt_df_noised))\n",
    "    \n",
    "    # = compute score =\n",
    "    val_score_noised = score(gt_df_noised.apply(np.ceil), pred_df_noised, row_id_column_name='id')\n",
    "\n",
    "    print(f\"Eval fold {f}: Val = {val_score}  Val noised = {val_score_noised}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19f428c4-6139-4865-8a60-83b60ae19faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 5 ckpts in ../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_finetuning_5epocas.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_finetuning_5epocas/fold_2.ckpt',\n",
       " '../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_finetuning_5epocas/fold_3.ckpt',\n",
       " '../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_finetuning_5epocas/fold_4.ckpt',\n",
       " '../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_finetuning_5epocas/fold_1.ckpt',\n",
       " '../chpks/30sec_effnet_frompretrained_5sec_samewaveaug_nobackground_overfitting_finetuning_5epocas/fold_0.ckpt']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_list = glob(f'{CONFIG.checkpoint_dir}/*.ckpt')\n",
    "print(f'find {len(ckpt_list)} ckpts in {CONFIG.checkpoint_dir}.')\n",
    "#ckpt_list = [ckpt_list[-1]]\n",
    "ckpt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ccf20-8087-4330-81ba-ed84ff421212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == setting of onnx ==\n",
    "\n",
    "input_tensor = torch.randn(64, 128, 938)  # input shape\n",
    "input_names = ['x']\n",
    "output_names = ['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452ef10-2b68-4f21-8e6e-de58ab2f140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_ckpt_list = list()\n",
    "for ckpt_path in ckpt_list:\n",
    "    ckpt_name = os.path.basename(ckpt_path).split('.')[0]\n",
    "    # == init model ==\n",
    "    bird_model = MyModel()\n",
    "    \n",
    "    # == load ckpt ==\n",
    "    weights = torch.load(ckpt_path, map_location=torch.device('cpu'))['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "    bird_model.eval()\n",
    "    \n",
    "    # == convert to onnx ==\n",
    "    torch.onnx.export(bird_model, input_tensor, f\"{CONFIG.checkpoint_dir}/{ckpt_name}.onnx\", verbose=False, input_names=input_names, output_names=output_names)\n",
    "#     onnx_ckpt_list.append(f\"{ckpt_name}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0de5ee-4548-4004-ad81-88e312ee0ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
